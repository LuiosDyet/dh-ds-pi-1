{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Integrantes:\n",
                "1. Camila Coltriani\n",
                "2. Luis Dartayet\n",
                "3. Irania Fuentes\n",
                "4. Jonathan Fichelson\n",
                "5. Ornella Cevoli\n",
                "# Trabajo práctico  3: Modelos de clasificación "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Introducción: \n",
                "\n",
                "El dataset a usar proviene de una empresa de retail en línea cuyo modelo de negocio se sustenta sobre una plataforma que opera únicamente de manera digital. \n",
                "\n",
                "El objetivo de este trabajo es identificar clientes que puedan estar propensos a dejar de utilizar la plataforma en el corto plazo.  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Las librerías utilizadas en este documento son:\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.preprocessing import OneHotEncoder \n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.model_selection import cross_val_score, KFold\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.preprocessing import binarize\n",
                "\n",
                "%matplotlib inline\n",
                "from matplotlib import pyplot as plt\n",
                "from matplotlib.ticker import ScalarFormatter\n",
                "from matplotlib import gridspec\n",
                "import seaborn as sns\n",
                "sns.set()\n",
                "\n",
                "import statsmodels.api as sm\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "\n",
                "from sklearn import metrics\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.metrics import roc_curve\n",
                "from sklearn.metrics import roc_auc_score\n",
                "from sklearn.metrics import classification_report\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.metrics import precision_score\n",
                "from sklearn.metrics import recall_score\n",
                "from sklearn.metrics import f1_score\n",
                "\n",
                "from collections import ChainMap\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Booleano para mostrar o no los gráficos del análisis exploratorio de datos\n",
                "show_EDA_plots = True"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Carga de datos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data  = pd.read_csv('./data/Datos ML 2021 Q2.csv', sep=';')\n",
                "print(\"El dataset tiene {} filas y {} columnas\".format(data.shape[0], data.shape[1]))\n",
                "data.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Descripción del dataset"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### El dataset tiene las siguientes columnas:\n",
                "\n",
                "- CustomerID: ID del cliente\n",
                "- Churn: Columna que indica si el cliente dejó de usar la plataforma o no. 1 es que se da de baja.\n",
                "- CustomerTenure: Es el tiempo transcurrido desde el inicio de la relación con el cliente (en meses)\n",
                "- MainDeviceLogin: Dispositivo principal que utiliza el cliente para acceder a la plataforma\n",
                "- CityTier: Indicador del nivel de desarrollo de la ciudad donde vive el cliente\n",
                "- WarehouseToHome: Distancia desde el centro de distribución a la vivienda del cliente (en km)\n",
                "- MainPaymentMode: Método de pago más utilizado por el cliente\n",
                "- Gender: Género del cliente\n",
                "- HourSpendOnApp: Número de horas que el cliente ha pasado en la plataforma\n",
                "- DeviceRegistered: Número de dispositivos en los que el cliente ha accedido a la plataforma\n",
                "- PrefCategory: Categoría más común de las compras del cliente en el último mes\n",
                "- SatisfactionScore: Nivel de satisfacción del cliente con el servicio\n",
                "- MaritalStatus: Estado civil del cliente\n",
                "- NumberOfAddress: Número de direcciones diferentes registradas por el cliente\n",
                "- Complain: Si ha realizado reclamos\n",
                "- OrderAmountHikeFromlastYear: Incremento porcentual en la cantidad de compras con respecto al año anterior\n",
                "- CouponUsed: Número de cupones usados en el último mes\n",
                "- OrderCount: Número de compras realizadas en el último mes\n",
                "- DaySinceLastOrder: Cantidad de días desde la última compra\n",
                "- CashbackAmount: Promedio de reembolsos pedidos en el último mes"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Variable objetivo: Churn (termino empleado en marketing para hacer referencia a si un cliente deja de usar una aplicación y/o regresa)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exploración de datos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cantidad de datos y chequeo de datos faltantes\n",
                "data.info()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- Eliminacion de datos NAN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Eliminación de valores nan\n",
                "## Veamos cuantos valores nulos hay en cada columna\n",
                "data.isna().sum()\n",
                "## Los eliminamos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#los eliminamos\n",
                "data.dropna(inplace=True)\n",
                "\n",
                "display(data.isna().sum())\n",
                "\n",
                "print(\"El dataset limpio tiene {} filas y {} columnas\".format(data.shape[0], data.shape[1]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# data.describe()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- Distribucion de la variable objetivo *Churn*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Distribucion de la variable objetivo\n",
                "display(data['Churn'].value_counts())\n",
                "display((data['Churn'].value_counts(normalize=True)*100))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "La variable objetivo Churn se distribuye en una clase mayoritaria Churn = 0 (datos = 2802), la cual, corresponde a los clientes que permanecen en la plataforma y una la minoritaria Churn = 1 (datos = 564) que corresponde a los clientes que se dan de baja.\n",
                "\n",
                "    Observamos que tenemos un dataset desbalanceado con un 83% de datos en la clase 0 y un 16% de datos para la clase 1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Presentamos la distribucion de datos unicos por columna\n",
                "for columnas in data.columns:\n",
                "    print(\"\")\n",
                "    print(f'Nombre:{columnas}')\n",
                "    print(data[columnas].value_counts())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- Detección de datos sospechosas o atípicos"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se consideran columnas con valores sospechosos aquellas cuya máximo valor se encuentran por encima de 3 desviaciones estándar de la media. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#definimos el limite para considerar un dato sospechoso\n",
                "std_limit = 3\n",
                "#Por la naturaleza de las variables, se considera que los valores sospechosos son aquellos que se encuentran por encima y no los inferiores."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Se crea una funcion para identificar los Columnas con datos sospechosas\n",
                "suspicious_columns = []\n",
                "\n",
                "for col in data.columns:\n",
                "    if(data[col].dtype == 'object'):\n",
                "        continue\n",
                "    mean = data[col].mean()\n",
                "    std = data[col].std()\n",
                "    max = data[col].max()\n",
                "    if(max > mean + std_limit*std):\n",
                "        suspicious_columns.append(data[col].name)\n",
                "suspicious_columns\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Se crea una funcion para identificar las filas con datos sospechosas y se grafican\n",
                "suspicious_rows_arr = []\n",
                "\n",
                "def investigate_suspicious_column(data, column, watch_outliers=True):\n",
                "    if(show_EDA_plots):\n",
                "        fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
                "        plt.suptitle(column)\n",
                "        sns.histplot(data[column], ax=ax[0])\n",
                "        sns.boxplot(data=data[column], ax=ax[1], orient='h')\n",
                "\n",
                "        plt.show()\n",
                "\n",
                "    if(watch_outliers):\n",
                "        mean = data[column].mean()\n",
                "        std = data[column].std()\n",
                "        max = data[column].max()\n",
                "\n",
                "        suspicious_rows = data[data[column] > mean + std_limit*std]\n",
                "        suspicious_rows_arr.append(suspicious_rows)\n",
                "        display(\"Hay {} filas sospechosas\".format(suspicious_rows.shape[0]))\n",
                "        display(suspicious_rows)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for col in suspicious_columns:\n",
                "    investigate_suspicious_column(data, col)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Total de filas sospechosas\n",
                "print(\"Hay {} filas sospechosas\".format(sum([suspicious_rows.shape[0] for suspicious_rows in suspicious_rows_arr])))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Filas sospechosas agrupadas por columna churn\n",
                "suspicious_rows = pd.concat(suspicious_rows_arr)\n",
                "display(suspicious_rows['Churn'].value_counts())\n",
                "display(suspicious_rows['Churn'].value_counts(normalize=True))\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Observamos que la distribución de la variable *Churn* entre los valores extremos es similar, por lo que no parece haber una relación entre las filas con estos datos y la variable objetivo.\n",
                "\n",
                "Por otro lado, haciendo una observación pormenorizada, creemos que en los casos de las columnas `CouponUsed`, `OrderCount` y `DaySinceLastOrder`  y `CashbackAmount` parecen ser valores lógicos, aún tratándose de valores extremos por lo que no las eliminaremos del dataset original."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Presentamos las columnas que fueron consideradas como sospechosas y las eliminamos de lista: ya no son más columnas sospechosas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(suspicious_columns)\n",
                "suspicious_columns.remove('CouponUsed')\n",
                "suspicious_columns.remove('OrderCount')\n",
                "suspicious_columns.remove('DaySinceLastOrder')\n",
                "suspicious_columns.remove('CashbackAmount')\n",
                "\n",
                "suspicious_columns"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Eliminamos los datos atípicos de las columnas que quedaron como sospechosas:\n",
                "\n",
                "CustomerTenure, WarehouseToHome, NumberOfAddress"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def remove_outliers(data, column):\n",
                "    mean = data[column].mean()\n",
                "    std = data[column].std()\n",
                "    max = data[column].max()\n",
                "    return data[data[column] <= mean + std_limit*std]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for col in suspicious_columns:\n",
                "    data = remove_outliers(data, col)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Veamos el resultado graficamente"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for col in suspicious_columns:\n",
                "    investigate_suspicious_column(data, col, False)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Correlación entre las variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Presentamos los nombres de las 20 columnas\n",
                "data.columns"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- Podemos inferir que las variables que tendrán una significancia en predecir el valor del churn son aquellas en las cuales se puedan observar diferencias en la distribución de las clase de *Churn*: Clase 0 y clase 1.\n",
                "\n",
                "    Graficamos las variables contra los valores 0 y 1 de *Churn* para verificar esta hipotesis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if(show_EDA_plots):\n",
                "    fig, axes = plt.subplots(nrows=7, ncols=3, figsize=(32,32))\n",
                "    fig.suptitle('Histogramas normalizados')\n",
                "    for c, ax in zip(data.columns, axes.flatten()):\n",
                "        sns.histplot(data = data.loc[data['Churn']==0, c].dropna(), stat = 'density', ax = ax, kde = False )\n",
                "        sns.histplot(data = data.loc[data['Churn']==1, c].dropna(), stat = 'density', kde=False, ax=ax, color = 'orange')\n",
                "        ax.legend(['Churn = 0', 'Churn = 1'])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "De los histogramas podemos observar que: *CustomerID*, *Gender*, *NumberOfAddress* son variables que no permiten diferenciar si el cliente se queda en la plataforma (clase 0) o la abandona (clase 1).\n",
                "\n",
                "Puede observarse que *CouponUsed* muestra una tendencia a predecir si los datos pertenecen a la clase 0 o clase 1 a medida que aumenta el numero de cupones usados. *OrderCount* por su parte no muestra esta tendencia clara cuando aumenta el numero de compras.\n",
                "\n",
                "\n",
                "Descartarmos las columnas que no permiten diferenciar las clases en *Churn*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Eliminamos las columnas que no permiten diferenciar las clases\n",
                "columns_to_eliminate = ['CustomerID', 'Gender', 'NumberOfAddress'] ##ojo parece que numberofadress si diferencoa\n",
                "\n",
                "data = data.drop(columns_to_eliminate, axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# if(show_EDA_plots):\n",
                "#     sns.pairplot(data, hue='Churn')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- Ahora queremos identificar si las variables presentan correlacion entre ellas y con la variable objetivo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vemos que variables están correlacionadas con Churn\n",
                "if(show_EDA_plots):\n",
                "    plt.figure(figsize=(20,10))\n",
                "    sns.heatmap(data.corr(), annot=True, vmin=-1, cmap='Blues')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#graficamos con pairplot las variables\n",
                "if(show_EDA_plots):\n",
                "    sns.pairplot(data)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Interpretacion de la mapa de la correlacion grafica y numerica:\n",
                "\n",
                "Las variables que muestran correlación (mapa de calor) con la variable objetivo **Churn** son:\n",
                "\n",
                "- *CustomerTenure* (-0.34), presenta una baja relación, sin embargo, nos indica que es negativa. Es decir, a medida que pasa el tiempo los clientes pueden permanecer en la plataforma o no sin darse de baja. \n",
                "\n",
                "- *Complain* (0.24), presenta baja relacion, graficamente nos muestra que los clientes que han realizado más reclamos se han quedado en comparación con los que han dejado la plataforma.\n",
                "\n",
                "Variables relacionadas entre ellas:\n",
                "\n",
                "- *OrderCount y CouponUsed* = 0.73, presentan una alta correlación, ya que, es logico que al usar más cupones de compra el numero de ordenes se vea incrementado. \n",
                "\n",
                "- *DaySinceLastOrder* y *OrderCount* = 0.45, presentan una buena correlacion, al pasar los dias desde la ultima compra es lógico que las ordenes incrementen. \n",
                "\n",
                "- *DaySinceLastOrder* y *CouponUsed* = 0.32, es la misma relacion que con OrderCount\n",
                "\n",
                "- *CustomerTenure* vs *cashbackAmount* =  0.22, presentan una baja relacion, es logico pensar que si los clientes se quedan más tiempo en la plataforma puede estar asociado a solicitudes de reembolsos."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "De esta interpretacion, se puede decidir eliminar como columna predictora *OrderCount* y *DaySinceLastOrder* por estar correlacionas altamente con *CouponUsed*. \n",
                "\n",
                "Se decide dejar *CoupondUsed* ya que graficamente (histogramas) se observa que puede diferenciar entre la clase cero y clase 1, mucho mejor que OrderCount."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Variables a eliminar\n",
                "columns_to_eliminate_2 = ['OrderCount', \"DaySinceLastOrder\"]\n",
                "\n",
                "data = data.drop(columns_to_eliminate_2, axis=1)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Estadisticamente se considera que una correlacion es despreciable si: r < |0.1|\n",
                "Vamos a eliminar las variables cuya correlación sea menor a 0.1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ## Correlación entre variables y churn\n",
                "abs_corr = data.corr()[['Churn']].abs().sort_values(by='Churn', ascending=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Correlación entre variables y churn, absoluta (para ordenar sin tener en cuenta si es positiva o negativa)\n",
                "if(show_EDA_plots):\n",
                "    plt.figure(figsize=(8,12))\n",
                "    sns.heatmap(abs_corr, annot=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Columnas a modelar: \", data.columns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Ordenamos las columnas con datos categoricos\n",
                "#Para PrefCategory"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Nos quedamos con las variables que tienen una correlación mayor a 0.1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Variables con correlación mayor a \n",
                "high_corr_vars = abs_corr[abs_corr['Churn'] >= 0.1]\n",
                "## Lo convertimos en una lista para poder iterar sobre ella\n",
                "high_corr_vars = high_corr_vars.index.tolist()\n",
                "high_corr_vars"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Modificamos nuestro dataset con las columnas de la lista que tienen correlación mayor a 0.1\n",
                "data = data[high_corr_vars]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*DE AQUI EN ADELANTE NO LO TOQUE*"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Separación de datos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = data.drop(['Churn'], axis=1)\n",
                "y = data['Churn']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.2)\n",
                "print(X_train.shape)\n",
                "print(X_test.shape)\n",
                "print(y_train.shape)\n",
                "print(y_test.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verificamos que la proporción de churn sea similar en los conjuntos de entrenamiento y testeo\n",
                "\n",
                "print(\"Churn en el conjunto de entrenamiento\")\n",
                "display(y_train.value_counts(normalize=True))\n",
                "print(\"Churn en el conjunto de testeo\")\n",
                "display(y_test.value_counts(normalize=True))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preparación de datos"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Identificamos las columnas categoricas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "categorical_columns = [col for col in data.columns if data[col].dtypes == 'object']\n",
                "\n",
                "categorical_columns"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Identificamos las columnas numericas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "numerical_columns = [col for col in data.columns if data[col].dtypes != 'object']\n",
                "\n",
                "numerical_columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DeviceRegistered es una variable categórica \n",
                "\n",
                "numerical_columns.remove('DeviceRegistered')\n",
                "categorical_columns.append('DeviceRegistered')\n",
                "\n",
                "# La transformamos a categórica\n",
                "\n",
                "X_train['DeviceRegistered'] = X_train['DeviceRegistered'].astype('object')\n",
                "X_test['DeviceRegistered'] = X_test['DeviceRegistered'].astype('object')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Complain es una variable categórica binaria: ha realizado reclamos, no ha realizado reclamos\n",
                "\n",
                "numerical_columns.remove('Complain')\n",
                "categorical_columns.append('Complain')\n",
                "\n",
                "# La transformamos a categórica\n",
                "\n",
                "X_train['Complain'] = X_train['Complain'].astype('object')\n",
                "X_test['Complain'] = X_test['Complain'].astype('object')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display('categorical_columns',categorical_columns)\n",
                "display('numerical_columns',numerical_columns)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Variables categóricas: \n",
                "Realizamos un OneHotEncoder para convertirlas en numericas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "encoder_categories = []\n",
                "\n",
                "X_categorical_columns = [x for x in categorical_columns]\n",
                "\n",
                "for col in X_categorical_columns:    \n",
                "    col_categories = data[col].unique()\n",
                "    encoder_categories.append(col_categories)\n",
                "\n",
                "encoder_categories"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "encoder = OneHotEncoder(categories = encoder_categories, sparse=False, drop='first')\n",
                "\n",
                "encoder = encoder.fit(X_train[X_categorical_columns])\n",
                "\n",
                "X_train_encoded = encoder.transform(X_train[X_categorical_columns])\n",
                "X_train_categorical = pd.DataFrame(X_train_encoded, columns = encoder.get_feature_names_out(X_categorical_columns))\n",
                "\n",
                "X_test_encoded = encoder.transform(X_test[X_categorical_columns])\n",
                "X_test_categorical = pd.DataFrame(X_test_encoded, columns = encoder.get_feature_names_out(X_categorical_columns))\n",
                "X_test_categorical.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Variables numéricas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train_numerical = X_train.drop(X_categorical_columns, axis=1)\n",
                "X_test_numerical = X_test.drop(X_categorical_columns, axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scaler = StandardScaler()\n",
                "\n",
                "X_train_scaled = scaler.fit_transform(X_train_numerical)\n",
                "X_train_numerical = pd.DataFrame(X_train_scaled, columns = X_train_numerical.columns)\n",
                "\n",
                "X_test_scaled = scaler.transform(X_test_numerical)\n",
                "X_test_numerical = pd.DataFrame(X_test_scaled, columns = X_test_numerical.columns)\n",
                "X_test_numerical.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Unimos las variables numéricas y categóricas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = pd.concat([X_train_categorical, X_train_numerical], axis=1)\n",
                "X_test = pd.concat([X_test_categorical, X_test_numerical], axis=1)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "PENDIENTE- Probar con otro DF, con alguna variable transformada. "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Utilizamos como variables predictoras *CustomerTenure* y *Complain_0*\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = X_train[['CustomerTenure', 'Complain_0']]\n",
                "X_test = X_test[['CustomerTenure', 'Complain_0']]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(X_train.shape)\n",
                "print(X_test.shape)\n",
                "print(y_train.shape)\n",
                "print(y_test.shape)\n",
                "\n",
                "X_train.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# null accuracy - accuracy predicha por un modelo que predice siempre la clase mayoritaria\n",
                "\n",
                "# total = y_test.shape[0]\n",
                "# tn = y_test.value_counts()[0]\n",
                "# fn = y_test.value_counts()[1]\n",
                "# tp = 0\n",
                "# fp = 0\n",
                "# null_accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
                "# print(\"TN: {}\".format(tn))\n",
                "# print(\"FN: {}\".format(fn))\n",
                "# print(\"Null accuracy: {}\".format(null_accuracy))\n",
                "\n",
                "# También podemos calcularlo con la siguiente función\n",
                "y_test.value_counts(normalize=True).max()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Utils para modelos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_confusion_matrixes(y_test, model_pred_dict):\n",
                "    fix, ax = plt.subplots(1, len(model_pred_dict), figsize=(16.5,4 ))\n",
                "    for i, model_name in enumerate(model_pred_dict):\n",
                "        y_pred = model_pred_dict[model_name]['y_pred']\n",
                "        conf_mat = confusion_matrix(y_test, y_pred)\n",
                "        conf_mat_df = pd.DataFrame(conf_mat, index = ['Negative (No Churn)', 'Positive (Churn)'], columns = ['Negative (No Churn)', 'Positive (Churn)'])\n",
                "        sns.heatmap(conf_mat_df, annot=True, fmt='g', cmap='Blues', ax=ax[i])\n",
                "        ax[i].set_title(model_name)\n",
                "        ax[i].set_ylabel('True')\n",
                "        ax[i].set_xlabel('Predicted')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_metrics(y_test, model_pred_dict):\n",
                "    metrics_list = []\n",
                "    for model_name in model_pred_dict:\n",
                "        y_pred = model_pred_dict[model_name]['y_pred']\n",
                "        tn = confusion_matrix(y_test, y_pred)[0,0]\n",
                "        fp = confusion_matrix(y_test, y_pred)[0,1]\n",
                "        tp = confusion_matrix(y_test, y_pred)[1,1]\n",
                "        fn = confusion_matrix(y_test, y_pred)[1,0]\n",
                "        metrics = pd.DataFrame({'Accuracy': [accuracy_score(y_test, y_pred)], \\\n",
                "                                'Recall': [recall_score(y_test, y_pred)], \\\n",
                "                                'Precision': [precision_score(y_test, y_pred)], \\\n",
                "                                'Specificity': [tn / (tn + fp)], \\\n",
                "                                'F1 score': [f1_score(y_test, y_pred)]})\n",
                "        metrics.index = [model_name]\n",
                "        metrics_list.append(metrics)\n",
                "        \n",
                "\n",
                "    df = pd.concat(metrics_list, axis=0)\n",
                "    return df\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_roc_curves(y_test, model_pred_dict):\n",
                "    for model_name in model_pred_dict:\n",
                "        y_pred_proba = model_pred_dict[model_name]['y_pred_proba']\n",
                "        fpr_log,tpr_log,thr_log = roc_curve(y_test, y_pred_proba[:,1])\n",
                "        plt.plot([0, 1], [0, 1], 'k--')\n",
                "        plt.plot(fpr_log, tpr_log, label=model_name)\n",
                "        plt.xlabel('False Positive Rate')\n",
                "        plt.ylabel('True Positive Rate')\n",
                "        plt.title('ROC Curve')\n",
                "        plt.legend()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_auc(y_test, model_pred_dict):\n",
                "    auc_list = []\n",
                "    for model_name in model_pred_dict:\n",
                "        y_pred_proba = model_pred_dict[model_name]['y_pred_proba']\n",
                "        auc = roc_auc_score(y_test, y_pred_proba[:,1])\n",
                "        auc_list.append(auc)\n",
                "    df = pd.DataFrame({'AUC': auc_list}, index=model_pred_dict)\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_thresholds(y_test, y_pred, y_pred_proba):\n",
                "    tn = confusion_matrix(y_test, y_pred)[0,0]\n",
                "    fp = confusion_matrix(y_test, y_pred)[0,1]\n",
                "\n",
                "    thresholds = np.arange(0, 1, 0.01)\n",
                "    accuracy = []\n",
                "    recall = []\n",
                "    precision = []\n",
                "    specificity = []\n",
                "    f1 = []\n",
                "    for i in thresholds:\n",
                "        y_pred = binarize(y_pred_proba, threshold=i)[:,1]\n",
                "        accuracy.append(accuracy_score(y_test, y_pred))\n",
                "        recall.append(recall_score(y_test, y_pred))\n",
                "        precision.append(precision_score(y_test, y_pred, zero_division=0))\n",
                "        specificity.append(tn / (tn + fp))\n",
                "        f1.append(f1_score(y_test, y_pred))\n",
                "    plt.plot(thresholds, accuracy, label='accuracy')\n",
                "    plt.plot(thresholds, recall, label='recall')\n",
                "    plt.plot(thresholds, precision, label='precision')\n",
                "    plt.plot(thresholds, specificity, label='specificity')\n",
                "    plt.plot(thresholds, f1, label='f1')\n",
                "    plt.legend()\n",
                "    plt.xlabel('threshold')\n",
                "    plt.ylabel('score')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_metric_thresholds(y_test, model_pred_dict, method):\n",
                "    for model_name in model_pred_dict:\n",
                "        y_pred = model_pred_dict[model_name]['y_pred']\n",
                "        y_pred_proba = model_pred_dict[model_name]['y_pred_proba']\n",
                "        tn = confusion_matrix(y_test, y_pred)[0,0]\n",
                "        fp = confusion_matrix(y_test, y_pred)[0,1]\n",
                "        thresholds = np.arange(0, 1, 0.01)\n",
                "        metric = []\n",
                "        for i in thresholds:\n",
                "            y_pred = binarize(y_pred_proba, threshold=i)[:,1]\n",
                "            metric.append(method(y_test, y_pred))\n",
                "        plt.plot(thresholds, metric, label=model_name)\n",
                "        plt.legend()\n",
                "        plt.xlabel('threshold')\n",
                "        plt.ylabel('score')   "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Diccionario de modelos\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_pred_dict = {}\n",
                "\n",
                "def add_to_model_pred_dict(model_name, y_pred, y_pred_proba):\n",
                "    model_pred_dict[model_name] = {'y_pred': y_pred, 'y_pred_proba': y_pred_proba}"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Naive Bayes"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gnb = GaussianNB()\n",
                "\n",
                "gnb.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_nb = gnb.predict(X_test)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_proba_nb = gnb.predict_proba(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "add_to_model_pred_dict('GNB', y_pred_nb, y_pred_proba_nb)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## KNN"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Utilizamos grid search para encontrar los mejores parámetros\n",
                "\n",
                "KNN = KNeighborsClassifier()\n",
                "\n",
                "k_range = list(range(2, 31))\n",
                "param_grid = dict(n_neighbors=k_range)\n",
                "print(param_grid)\n",
                "\n",
                "folds=StratifiedKFold(n_splits=10, shuffle=True)\n",
                "\n",
                "grid = GridSearchCV(KNN, param_grid, cv=folds, scoring='recall')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid.best_estimator_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid.best_score_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid.best_params_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_knn = grid.predict(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_proba_knn = grid.predict_proba(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "add_to_model_pred_dict('KNN', y_pred_knn, y_pred_proba_knn)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Regresión logística"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Utilizamos grid search para encontrar los mejores parámetros\n",
                "\n",
                "lr = LogisticRegression()\n",
                "\n",
                "c_range = np.logspace(-2, 4, 7)\n",
                "\n",
                "param_grid = dict(C=c_range)\n",
                "\n",
                "grid = GridSearchCV(lr, param_grid, cv=folds, scoring='recall')\n",
                "\n",
                "grid.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid.best_estimator_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid.best_score_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid.best_params_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_lr = grid.predict(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_proba_lr = grid.predict_proba(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "add_to_model_pred_dict('LR', y_pred_lr, y_pred_proba_lr)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Métricas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "create_confusion_matrixes(y_test, model_pred_dict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "metrics = compare_metrics(y_test, model_pred_dict)\n",
                "\n",
                "display(metrics)\n",
                "\n",
                "# Plot metrics\n",
                "\n",
                "metrics.plot(kind='bar', figsize=(10, 4), xlabel='Model', ylabel='Score', rot=0)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "compare_roc_curves(y_test, model_pred_dict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "compare_auc(y_test, model_pred_dict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "compare_metric_thresholds(y_test, model_pred_dict, method=recall_score)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusiones - PENDIENTE"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- Los datos modelados tienen un 85% de casos con clientes que se no abandonan la app y 15 % de clientes que se dan de baja, por lo cual, consideramos que representa un alto sesgo para el modelo."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "DH",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.4"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "fa08644e93f5ad6b3c1e930965e944c16707fd43381b34471e0217a3cc73ebe0"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
