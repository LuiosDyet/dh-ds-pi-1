{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Integrantes:\n",
                "1. Camila Coltriani\n",
                "2. Luis Dartayet\n",
                "3. Irania Fuentes\n",
                "4. Jonathan Fichelson\n",
                "5. Ornella Cevoli\n",
                "# Trabajo práctico  3: Modelos de clasificación "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Introducción y objetivo"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "El objetivo de este trabajo es predecir utilizando modelos de clasificacion si un cliente se dará de baja o no de la plataforma."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Las librerías utilizadas en este documento son:\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.preprocessing import OneHotEncoder \n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.model_selection import cross_val_score, KFold\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.preprocessing import binarize\n",
                "\n",
                "%matplotlib inline\n",
                "from matplotlib import pyplot as plt\n",
                "from matplotlib.ticker import ScalarFormatter\n",
                "from matplotlib import gridspec\n",
                "import seaborn as sns\n",
                "sns.set()\n",
                "\n",
                "import statsmodels.api as sm\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "\n",
                "from sklearn import metrics\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.metrics import roc_curve\n",
                "from sklearn.metrics import roc_auc_score\n",
                "from sklearn.metrics import classification_report\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.metrics import precision_score\n",
                "from sklearn.metrics import recall_score\n",
                "from sklearn.metrics import f1_score\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Carga de datos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data  = pd.read_csv('./data/Datos ML 2021 Q2.csv', sep=';')\n",
                "print(\"El dataset tiene {} filas y {} columnas\".format(data.shape[0], data.shape[1]))\n",
                "data.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Descripción del dataset"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### El dataset tiene las siguientes columnas:\n",
                "\n",
                "- CustomerID: ID del cliente\n",
                "- Churn: Columna que indica si el cliente dejó de usar la plataforma o no. 1 es que se da de baja.\n",
                "- CustomerTenure: Es el tiempo transcurrido desde el inicio de la relación con el cliente (en meses)\n",
                "- MainDeviceLogin: Dispositivo principal que utiliza el cliente para acceder a la plataforma\n",
                "- CityTier: Indicador del nivel de desarrollo de la ciudad donde vive el cliente\n",
                "- WarehouseToHome: Distancia desde el centro de distribución a la vivienda del cliente (en km)\n",
                "- MainPaymentMode: Método de pago más utilizado por el cliente\n",
                "- Gender: Género del cliente\n",
                "- HourSpendOnApp: Número de horas que el cliente ha pasado en la plataforma\n",
                "- DeviceRegistered: Número de dispositivos en los que el cliente ha accedido a la plataforma\n",
                "- PrefCategory: Categoría más común de las compras del cliente en el último mes\n",
                "- SatisfactionScore: Nivel de satisfacción del cliente con el servicio\n",
                "- MaritalStatus: Estado civil del cliente\n",
                "- NumberOfAddress: Número de direcciones diferentes registradas por el cliente\n",
                "- Complain: Si ha realizado reclamos\n",
                "- OrderAmountHikeFromlastYear: Incremento porcentual en la cantidad de compras con respecto al año anterior\n",
                "- CouponUsed: Número de cupones usados en el último mes\n",
                "- OrderCount: Número de compras realizadas en el último mes\n",
                "- DaySinceLastOrder: Cantidad de días desde la última compra\n",
                "- CashbackAmount: Promedio de reembolsos pedidos en el último mes"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Variable objetivo: Churn (termino empleado en marketing para hacer referencia a si un cliente deja de usar una aplicación y/o regresa)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exploración de datos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(data['Churn'].value_counts())\n",
                "display(data['Churn'].value_counts(normalize=True))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Detección de datos sospechosas o atípicos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##Se consideran columnas con valores sospechosos aquellas cuya máxima valor se encuentran por encima de 3 desviaciones estándar de la media. \n",
                "std_limit = 3\n",
                "##Por la naturaleza de las variables, se considera que los valores sospechosos son aquellos que se encuentran por encima y no los inferiores."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Columnas sospechosas\n",
                "\n",
                "suspicious_columns = []\n",
                "\n",
                "for col in data.columns:\n",
                "    if(data[col].dtype == 'object'):\n",
                "        continue\n",
                "    mean = data[col].mean()\n",
                "    std = data[col].std()\n",
                "    max = data[col].max()\n",
                "    if(max > mean + std_limit*std):\n",
                "        suspicious_columns.append(data[col].name)\n",
                "suspicious_columns\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "suspicious_rows_arr = []\n",
                "\n",
                "def investigate_suspicious_column(data, column, watch_outliers=True):\n",
                "    fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
                "    plt.suptitle(column)\n",
                "    sns.histplot(data[column], ax=ax[0])\n",
                "    sns.boxplot(data=data[column], ax=ax[1], orient='h')\n",
                "\n",
                "    plt.show()\n",
                "\n",
                "    if(watch_outliers):\n",
                "        mean = data[column].mean()\n",
                "        std = data[column].std()\n",
                "        max = data[column].max()\n",
                "\n",
                "        suspicious_rows = data[data[column] > mean + std_limit*std]\n",
                "        suspicious_rows_arr.append(suspicious_rows)\n",
                "        display(\"Hay {} filas sospechosas\".format(suspicious_rows.shape[0]))\n",
                "        display(suspicious_rows)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for col in suspicious_columns:\n",
                "    investigate_suspicious_column(data, col)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Total de filas sospechosas\n",
                "print(\"Hay {} filas sospechosas\".format(sum([suspicious_rows.shape[0] for suspicious_rows in suspicious_rows_arr])))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Filas sospechosas agrupadas por columna churn\n",
                "\n",
                "suspicious_rows = pd.concat(suspicious_rows_arr)\n",
                "display(suspicious_rows['Churn'].value_counts())\n",
                "display(suspicious_rows['Churn'].value_counts(normalize=True))\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Observamos que la distribución de la variable Churn entre los valores extremos es similar, por lo que no parece haber una relación entre las filas con estos datos y la variable objetivo.\n",
                "\n",
                "Por otro lado, haciendo una observación pormenorizada, creemos que en los casos de las columnas `CouponUsed`, `OrderCount` y `DaySinceLastOrder`  y `CashbackAmount` parecen ser valores lógicos, aún tratándose de valores extremos por lo que no las eliminaremos del dataset original."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "suspicious_columns"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Removemos las columnas mencionadas de la lista de columnas sospechosas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "suspicious_columns.remove('CouponUsed')\n",
                "suspicious_columns.remove('OrderCount')\n",
                "suspicious_columns.remove('DaySinceLastOrder')\n",
                "suspicious_columns.remove('CashbackAmount')\n",
                "\n",
                "suspicious_columns"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Limpiemos las filas sospechosas de las columnas que quedaron como sospechosas:\n",
                "\n",
                "CustomerTenure, WarehouseToHome, NumberOfAddress"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def remove_outliers(data, column):\n",
                "    mean = data[column].mean()\n",
                "    std = data[column].std()\n",
                "    max = data[column].max()\n",
                "    return data[data[column] <= mean + std_limit*std]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for col in suspicious_columns:\n",
                "    data = remove_outliers(data, col)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Veamos el resultado"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for col in suspicious_columns:\n",
                "    investigate_suspicious_column(data, col, False)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Correlación de las variables"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "PENDIENTE- PAIR PLOT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(nrows=7, ncols=3, figsize=(32,32))\n",
                "fig.suptitle('Histogramas normalizados')\n",
                "for c, ax in zip(data.columns, axes.flatten()):\n",
                "    sns.histplot(data = data.loc[data['Churn']==0, c].dropna(), stat = 'density', ax = ax, kde = False )\n",
                "    sns.histplot(data = data.loc[data['Churn']==1, c].dropna(), stat = 'density', kde=False, ax=ax, color = 'orange')\n",
                "    ax.legend(['Churn = 0', 'Churn = 1'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sns.pairplot(data, hue='Churn')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "PENDIENTE -Analizar pairplot\n",
                "\n",
                "Me está diciendo que no hay correlación entre las variables, pero no estoy seguro de que sea así.\n",
                "\n",
                "excepción sería la correlación entre `OrderCount` y `CouponUsed` "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Podemos inferir que las variables que tendrán una significancia en establecer el valor del churn son aquellas en las cuales podemos observar diferencias en la distribución de los valores de churn.\n",
                "\n",
                "Por lo tanto podemos descartar las variables que no presentan diferencias en la distribución de los valores de churn."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "De los graficos podemos observar que: CustomerID, Gender, NumberOfAddress son las columnas que no permiten diferenciar lo mencionado anteriormente"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Variables a eliminar\n",
                "columns_to_eliminate = ['CustomerID', 'Gender', 'NumberOfAddress']\n",
                "\n",
                "data = data.drop(columns_to_eliminate, axis=1)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Veamos la correlación entre las variables"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "PENDIENTE- buscar correlación entre variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(20,10))\n",
                "sns.heatmap(data.corr(), annot=True, vmin=-1, cmap='Blues')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Correlación entre variables y churn\n",
                "abs_corr = data.corr()[['Churn']].abs().sort_values(by='Churn', ascending=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Correlación entre variables y churn, absoluta (para ordenar sin tener en cuenta si es positiva o negativa)\n",
                "\n",
                "plt.figure(figsize=(8,12))\n",
                "sns.heatmap(abs_corr, annot=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Nos quedamos con las variables que tienen una correlación mayor a 0.1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Variables con correlación mayor a 0.1\n",
                "\n",
                "high_corr_vars = abs_corr[abs_corr['Churn'] > 0.1]\n",
                "\n",
                "## Lo convertimos en una lista para poder iterar sobre ella\n",
                "\n",
                "high_corr_vars = high_corr_vars.index.tolist()\n",
                "\n",
                "high_corr_vars"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Modificamos nuestro dataset con las columnas de la lista que tienen correlación mayor a 0.1\n",
                "data = data[high_corr_vars]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Eliminación de valores nan"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Veamos cuantos valores nulos hay en cada columna\n",
                "\n",
                "data.isna().sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Los eliminamos\n",
                "\n",
                "data.dropna(inplace=True)\n",
                "\n",
                "display(data.isna().sum())\n",
                "\n",
                "print(\"El dataset limpio tiene {} filas y {} columnas\".format(data.shape[0], data.shape[1]))\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Observemos la distribución de los valores de churn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(data['Churn'].value_counts())\n",
                "display(data['Churn'].value_counts(normalize=True))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "El dataset está desbalanceado, por lo que se deberá tener en cuenta al momento de entrenar los modelos."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Separación de datos"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "PENDIENTE - estratificar"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = data.drop(['Churn'], axis=1)\n",
                "y = data['Churn']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.2)\n",
                "print(X_train.shape)\n",
                "print(X_test.shape)\n",
                "print(y_train.shape)\n",
                "print(y_test.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verificamos que la proporción de churn sea similar en los conjuntos de entrenamiento y testeo\n",
                "\n",
                "print(\"Churn en el conjunto de entrenamiento\")\n",
                "display(y_train.value_counts(normalize=True))\n",
                "print(\"Churn en el conjunto de testeo\")\n",
                "display(y_test.value_counts(normalize=True))\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preparación de datos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "categorical_columns = [col for col in data.columns if data[col].dtypes == 'object']\n",
                "\n",
                "categorical_columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "numerical_columns = [col for col in data.columns if data[col].dtypes != 'object']\n",
                "\n",
                "numerical_columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DeviceRegistered es una variable categórica \n",
                "\n",
                "numerical_columns.remove('DeviceRegistered')\n",
                "categorical_columns.append('DeviceRegistered')\n",
                "\n",
                "# La transformamos a categórica\n",
                "\n",
                "X_train['DeviceRegistered'] = X_train['DeviceRegistered'].astype('object')\n",
                "X_test['DeviceRegistered'] = X_test['DeviceRegistered'].astype('object')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Complain es una variable categórica binaria \n",
                "\n",
                "numerical_columns.remove('Complain')\n",
                "categorical_columns.append('Complain')\n",
                "\n",
                "# La transformamos a categórica\n",
                "\n",
                "X_train['Complain'] = X_train['Complain'].astype('object')\n",
                "X_test['Complain'] = X_test['Complain'].astype('object')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display('categorical_columns',categorical_columns)\n",
                "display('numerical_columns',numerical_columns)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Variables categóricas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "encoder_categories = []\n",
                "\n",
                "X_categorical_columns = [x for x in categorical_columns]\n",
                "\n",
                "for col in X_categorical_columns:    \n",
                "    col_categories = data[col].unique()\n",
                "    encoder_categories.append(col_categories)\n",
                "\n",
                "encoder_categories"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "encoder = OneHotEncoder(categories = encoder_categories, sparse=False, drop='first')\n",
                "\n",
                "encoder = encoder.fit(X_train[X_categorical_columns])\n",
                "\n",
                "X_train_encoded = encoder.transform(X_train[X_categorical_columns])\n",
                "X_train_categorical = pd.DataFrame(X_train_encoded, columns = encoder.get_feature_names_out(X_categorical_columns))\n",
                "\n",
                "X_test_encoded = encoder.transform(X_test[X_categorical_columns])\n",
                "X_test_categorical = pd.DataFrame(X_test_encoded, columns = encoder.get_feature_names_out(X_categorical_columns))\n",
                "X_test_categorical.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Variables numéricas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train_numerical = X_train.drop(X_categorical_columns, axis=1)\n",
                "X_test_numerical = X_test.drop(X_categorical_columns, axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scaler = StandardScaler()\n",
                "\n",
                "X_train_scaled = scaler.fit_transform(X_train_numerical)\n",
                "X_train_numerical = pd.DataFrame(X_train_scaled, columns = X_train_numerical.columns)\n",
                "\n",
                "X_test_scaled = scaler.transform(X_test_numerical)\n",
                "X_test_numerical = pd.DataFrame(X_test_scaled, columns = X_test_numerical.columns)\n",
                "X_test_numerical.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Unimos las variables numéricas y categóricas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = pd.concat([X_train_categorical, X_train_numerical], axis=1)\n",
                "X_test = pd.concat([X_test_categorical, X_test_numerical], axis=1)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "PENDIENTE- Probar con otro DF, con alguna variable transformada. "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "PENDIENTE- explicar porque se eligen esas dos variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = X_train[['CustomerTenure', 'Complain_0']]\n",
                "X_test = X_test[['CustomerTenure', 'Complain_0']]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(X_train.shape)\n",
                "print(X_test.shape)\n",
                "print(y_train.shape)\n",
                "print(y_test.shape)\n",
                "\n",
                "X_train.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# null accuracy - accuracy predicha por un modelo que predice siempre la clase mayoritaria\n",
                "\n",
                "total = y_test.shape[0]\n",
                "tn = y_test.value_counts()[0]\n",
                "fn = y_test.value_counts()[1]\n",
                "tp = 0\n",
                "fp = 0\n",
                "null_accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
                "print(\"TN: {}\".format(tn))\n",
                "print(\"FN: {}\".format(fn))\n",
                "print(\"Null accuracy: {}\".format(null_accuracy))\n",
                "\n",
                "# También podemos calcularlo con la siguiente función\n",
                "# y_test.value_counts(normalize=True).max()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Utils para modelos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_confusion_matrix(y_test, y_pred):\n",
                "    \n",
                "    conf_mat = confusion_matrix(y_test, y_pred)\n",
                "    conf_mat_df = pd.DataFrame(conf_mat, index = ['Negative (No Churn)', 'Positive (Churn)'], columns = ['Negative (No Churn)', 'Positive (Churn)'])\n",
                "    plt.figure(figsize=(5.5,4))\n",
                "    sns.heatmap(conf_mat_df, annot=True, fmt='g', cmap='Blues')\n",
                "    plt.title('Matriz de confusión')\n",
                "    plt.ylabel('True')\n",
                "    plt.xlabel('Predicted')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_metrics(y_test, y_pred):\n",
                "    tn = confusion_matrix(y_test, y_pred)[0,0]\n",
                "    fp = confusion_matrix(y_test, y_pred)[0,1]\n",
                "\n",
                "\n",
                "    # accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
                "\n",
                "    print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n",
                "\n",
                "    # recall = tp / (tp + fn)\n",
                "\n",
                "    print('Recall: %.3f' % recall_score(y_test, y_pred))\n",
                "\n",
                "    # precision = tp / (tp + fp)\n",
                "\n",
                "    print('Precision: %.3f' % precision_score(y_test, y_pred))\n",
                "\n",
                "    # specificity = tn / (tn + fp)\n",
                "\n",
                "    print('Specificity: %.3f' % (tn / (tn + fp)))\n",
                "\n",
                "    # f1 = 2 * (precision * recall) / (precision + recall)\n",
                "\n",
                "    print('F1 score: %.3f' % f1_score(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_roc_curve(y_test, y_pred_proba):\n",
                "    \n",
                "    fpr_log,tpr_log,thr_log = roc_curve(y_test, y_pred_proba[:,1])\n",
                "    plt.plot([0, 1], [0, 1], 'k--')\n",
                "    plt.plot(fpr_log, tpr_log, label='GNB')\n",
                "    plt.xlabel('False Positive Rate')\n",
                "    plt.ylabel('True Positive Rate')\n",
                "    plt.title('GNB ROC Curve')\n",
                "    plt.show()\n",
                "\n",
                "    # AUC - Area Under the Curve\n",
                "\n",
                "    auc = roc_auc_score(y_test, y_pred_proba[:,1])\n",
                "    print('AUC: %.2f' % auc)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_thresholds(y_test, y_pred, y_pred_proba):\n",
                "    tn = confusion_matrix(y_test, y_pred)[0,0]\n",
                "    fp = confusion_matrix(y_test, y_pred)[0,1]\n",
                "\n",
                "    thresholds = np.arange(0, 1, 0.01)\n",
                "    accuracy = []\n",
                "    recall = []\n",
                "    precision = []\n",
                "    specificity = []\n",
                "    f1 = []\n",
                "    for i in thresholds:\n",
                "        y_pred = binarize(y_pred_proba, threshold=i)[:,1]\n",
                "        accuracy.append(accuracy_score(y_test, y_pred))\n",
                "        recall.append(recall_score(y_test, y_pred))\n",
                "        precision.append(precision_score(y_test, y_pred, zero_division=0))\n",
                "        specificity.append(tn / (tn + fp))\n",
                "        f1.append(f1_score(y_test, y_pred))\n",
                "    plt.plot(thresholds, accuracy, label='accuracy')\n",
                "    plt.plot(thresholds, recall, label='recall')\n",
                "    plt.plot(thresholds, precision, label='precision')\n",
                "    plt.plot(thresholds, specificity, label='specificity')\n",
                "    plt.plot(thresholds, f1, label='f1')\n",
                "    plt.legend()\n",
                "    plt.xlabel('threshold')\n",
                "    plt.ylabel('score')\n",
                "    plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Naive Bayes"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gnb = GaussianNB()\n",
                "\n",
                "gnb.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_nb = gnb.predict(X_test)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_proba_nb = gnb.predict_proba(X_test)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## KNN"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Utilizamos grid search para encontrar los mejores parámetros\n",
                "\n",
                "KNN = KNeighborsClassifier()\n",
                "\n",
                "k_range = list(range(1, 31))\n",
                "param_grid = dict(n_neighbors=k_range)\n",
                "print(param_grid)\n",
                "\n",
                "folds=StratifiedKFold(n_splits=10, random_state=19, shuffle=True)\n",
                "\n",
                "grid = GridSearchCV(KNN, param_grid, cv=folds, scoring='recall')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid.best_estimator_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid.best_score_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid.best_params_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_knn = grid.predict(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_proba_knn = grid.predict_proba(X_test)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Regresión logística"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Utilizamos grid search para encontrar los mejores parámetros\n",
                "\n",
                "lr = LogisticRegression()\n",
                "\n",
                "c_range = np.logspace(-2, 4, 7)\n",
                "\n",
                "param_grid = dict(C=c_range)\n",
                "\n",
                "grid = GridSearchCV(lr, param_grid, cv=folds, scoring='recall')\n",
                "\n",
                "grid.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid.best_estimator_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid.best_score_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid.best_params_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_lr = grid.predict(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_proba_lr = grid.predict_proba(X_test)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Métricas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Naive Bayes:\")\n",
                "create_confusion_matrix(y_test, y_pred_nb)\n",
                "print(\"KNN:\")\n",
                "create_confusion_matrix(y_test, y_pred_knn)\n",
                "print(\"Logistic Regression:\")\n",
                "create_confusion_matrix(y_test, y_pred_lr)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Naive Bayes:\")\n",
                "create_metrics(y_test, y_pred_nb)\n",
                "print(\"-------------\")\n",
                "print(\"KNN:\")\n",
                "create_metrics(y_test, y_pred_knn)\n",
                "print(\"-------------\")\n",
                "print(\"Logistic Regression:\")\n",
                "create_metrics(y_test, y_pred_lr)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Naive Bayes:\")\n",
                "create_roc_curve(y_test, y_pred_proba_nb)\n",
                "create_roc_curve(y_test, y_pred_proba_knn)\n",
                "create_roc_curve(y_test, y_pred_proba_lr)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "PENDIENTE - hacer en un mismo grafico las tres metricas de recall- explicar xq se usa recall"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "compare_thresholds(y_test, y_pred_nb, y_pred_proba_nb)\n",
                "compare_thresholds(y_test, y_pred_knn, y_pred_proba_knn)\n",
                "compare_thresholds(y_test, y_pred_lr, y_pred_proba_lr)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusiones - PENDIENTE"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- Los datos modelados tienen un 85% de casos con clientes que se no abandonan la app y 15 % de clientes que se dan de baja, por lo cual, consideramos que representa un alto sesgo para el modelo."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "dh",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.4"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "de0c11672bdc465268fe040a07375f6ad60f942d46756d33f7fe9e449a78b4ed"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
