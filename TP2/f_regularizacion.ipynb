{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Las librerías utilizadas en este documento son:\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from matplotlib import gridspec\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En esta sección evaluremos la selección de predictores utilizando regularización ridge y lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leemos el df creado para el barrio de caballito\n",
    "df = pd.read_csv('./data/data_limpio_gdf_caballito.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables predictoras: latitud, longitud y superficie total\n",
    "\n",
    "Variable objetivo: precio en usd\n",
    "\n",
    "Volvemos a presentar las metricas obtenidas en la regresión multiple con OLS para las variables mencionadas para comparar con las metricas obtenidas en Lasso y Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['lat', 'lon', 'sup_m2_total']]\n",
    "y = df['precio_usd']\n",
    "# Normalizamos los datos\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Dividimos en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "X_test_sm = scaler.transform(X_test)\n",
    "\n",
    "model = sm.OLS(y_train, X_train_sm).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un nivel de confianza de 0.05, el p-value del modelo es 0.7007 para la variable latitud por lo que se puede decir que el modelo es peor que lo esperado por el azar para esa variable, el coeficiente parcial de regresión es 0 y no tiene correlación con la varible objetivo.\n",
    "los p-value de longitud y superficie total (0.043 y 0.000) no están indicando que las variables contribuyen a la preddicción de nuestra variable objetivo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos con regularización ridge\n",
    "lm_ridge = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "\n",
    "model_ridge = lm_ridge.fit(X_train, y_train)\n",
    "\n",
    "print ('Modelo Ridge:')\n",
    "print('hiperparametro alpha: ', model_ridge.alpha_)\n",
    "print('coeficientes ajustados: ', model_ridge.coef_)\n",
    "print('intercepto: ', model_ridge.intercept_)\n",
    "print('R2 train: ', model_ridge.score(X_train, y_train))\n",
    "print('R2 test: ', model_ridge.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método consigue minimizar la influencia sobre el modelo de los predictores menos relacionados con la variable respuesta disminuyendolos sin llegar a cero.\n",
    "\n",
    "Vemos como al igual que el modelo OLS también nos indica que la variable latitud está muy poco correlacionada con el precio usd\n",
    "\n",
    "Existe mayor influencia de la variable superficie total que longitud al explicar la variable precio usd\n",
    "\n",
    "El valor promedio de la variable objetivo permanece sin mayor cambio en los modelos OLS y LASSO\n",
    "\n",
    "El R2 de test en Ridge explica el 43% de la variabilidad observada en la variable precio en usd.\n",
    "\n",
    "Los R2 de train y test son similares e indican que no hay overfitting y las pruebas en test resultan mejores que en train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos con regularización lasso\n",
    "\n",
    "lm_lasso = linear_model.LassoCV(alphas=np.logspace(-6, 6, 13), cv=5)\n",
    "\n",
    "model_lasso = lm_lasso.fit(X_train, y_train)\n",
    "\n",
    "print ('Modelo Lasso:')\n",
    "print('hiperparametro alpha : ', model_lasso.alpha_)\n",
    "print('coeficientes ajustados: ', model_lasso.coef_)\n",
    "print('intercepto: ', model_lasso.intercept_)\n",
    "print('R2 train: ', model_lasso.score(X_train, y_train))\n",
    "print('R2 test: ', model_lasso.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regularización con Lasso tiene el efecto de forzar a que los coeficientes de los predictores tiendan a cero; un predictor con coeficiente de regresión cero no influye en el modelo, por lo que consigue excluir los predictores que no son relevantes.\n",
    "\n",
    "En este caso la regularización con lasso no llevó ninguna variable a cero, sin embargo nos sigue mostrando una mejor explicación de precio usd con superficie total principalmente y longitud.\n",
    "\n",
    "Los R2 de train y test son similares e indican que no hay overfitting y las pruebas en test resultan mejores que en train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizaremos el impacto de la variables ambientes en la predicción de la variable precio. \n",
    "\n",
    "    \"ver la explicación al final de la sección\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_ambientes_cat = df[['ambientes_cat', 'precio_usd', 'municipio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos las variables dummies para la variable categórica de ambientes\n",
    "df_0_ambientes_cat = pd.get_dummies(df_0_ambientes_cat, columns=['ambientes_cat'], drop_first=True)\n",
    "print(df_0_ambientes_cat.shape)\n",
    "df_0_ambientes_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = pd.concat([df_0_ambientes_cat, df[\"sup_m2_total\"]],axis=1, join='inner')\n",
    "print(df_cat.shape)\n",
    "df_cat.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignamos las variables predictoras\n",
    "\n",
    "X_cat = df_cat[['ambientes_cat_1', 'ambientes_cat_2', 'ambientes_cat_3', 'ambientes_cat_4 o mas','sup_m2_total']]\n",
    "y = df_cat['precio_usd']\n",
    "\n",
    "# Normalizamos los datos\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_cat)\n",
    "X_cat = scaler.transform(X_cat)\n",
    "# X_cat\n",
    "\n",
    "# Dividimos en train y test\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat, y, test_size=0.2, random_state=42)\n",
    "#y_test_cat\n",
    "\n",
    "# Instanciamos el modelo y lo entrenamos\n",
    "lr_cat = linear_model.LinearRegression()\n",
    "lr_cat.fit(X_train_cat, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos los coeficientes\n",
    "\n",
    "print('Coeficientes: ', lr_cat.coef_)\n",
    "print('Intercepto: ', lr_cat.intercept_)\n",
    "\n",
    "# Calculamos el R2\n",
    "print('R2: ', r2_score(y_test_cat, lr_cat.predict(X_test_cat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelamos con statsmodels\n",
    "\n",
    "X_train_sm_2 = sm.add_constant(X_train_cat)\n",
    "model_stats = sm.OLS(y_train_cat, X_train_sm_2).fit()\n",
    "\n",
    "print(model_stats.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos con regularización ridge\n",
    "\n",
    "lm_ridge_2 = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13, 20,30))\n",
    "\n",
    "model_ridge_2 = lm_ridge_2.fit(X_train_cat, y_train_cat)\n",
    "\n",
    "print(lm_ridge_2.alpha_)\n",
    "print(lm_ridge_2.coef_)\n",
    "print(model_ridge_2.score(X_test_cat, y_test_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos con regularización lasso\n",
    "\n",
    "lm_lasso_2 = linear_model.LassoCV(alphas=np.logspace(-6, 6, 13,20,30), cv=5)\n",
    "\n",
    "model_lasso_2 = lm_lasso_2.fit(X_train_cat, y_train_cat)\n",
    "\n",
    "print(lm_lasso_2.alpha_)\n",
    "print(lm_lasso_2.coef_)\n",
    "print(model_lasso_2.score(X_test_cat, y_test_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al modelar las categorias ambientes obtenemos:\n",
    "\n",
    "- OLS: Para un nivel de confianza de 0.05, los p-value del modelo para las variables ambientes (1 a 4+) son mucho mayores al nivel de confianza por lo que se puede inferir que los coeficientes parciales de regresión son iguales a 0 y no tiene correlación con la varible objetivo.\n",
    "\n",
    "- Ridge:vemos que la influencia sobre el modelo de los predictores ambientes (1 ambiente a 3 ambientes) están muy poco relacionados con la variable respuesta por lo que los vemos disminuidos.\n",
    "\n",
    "   para mas de 4 ambientes y superficie vemos que existe correlación\n",
    "\n",
    "- Lasso: vemos que la influencia sobre el modelo de los predictores ambientes (1 ambiente a 2 ambientes) están muy poco relacionados con la variable respuesta por lo que los vemos disminuidos.\n",
    "   \n",
    "   No hay influencia de los immuebles de 3 ambientes con la variable precio pues el valor del coeficiente es cero.\n",
    "\n",
    "   La mayor respuesta para explicar el precio usd de las propiedades es la superficie total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DH')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa08644e93f5ad6b3c1e930965e944c16707fd43381b34471e0217a3cc73ebe0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
