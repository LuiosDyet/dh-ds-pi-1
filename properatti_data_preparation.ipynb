{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Trabajo práctico 1 : Analisis exploratorio del dataset Properatti\n",
                "\n",
                "Grupo #11: Camila Coltriani, Irania Fuentes, Johnatan Fischelson, Luis Dartayet, Ornela Cevolli  "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Para este trabajo utilizaremos un dataset de la inmobiliaría Properati construido con los datos de venta de propiedades en diferentes provincias de Argentina en el primer semestre del año 2017."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Identificar el problema ¿o cambiar por objetivos?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "El objetivo de este trabajo es realizar una limpieza del dataset properatti con la finalidad de obtener un dataset final con datos confiables que pueda ser utilizado en la generación de un modelo estadistico posterior.\n",
                "Con base en esto se plantean los siguientes objetivos especificos:\n",
                " - Adquirir los datos: leer y conocer su estructura para determinar las herramientas apropiadas para su manipulación.\n",
                "\n",
                " - Parsear los datos: realizar el analisis exploratorio de los datos que permita verificar la existencia o no de relaciones entre variables, valores duplicados, valores faltantes, valores atípicos o valores erroneos que para validar o aumentar la confiabilidad de los datos.\n",
                "\n",
                " - Minar los datos: aplicar las herramientas de python para corregir datos erroneos o duplicados, completar/eliminar valores nulos.\n",
                " \n",
                " - Refinar los datos: eliminar variables redundantes o repetidas, crear nuevas variables y dar un formato limpio al dataset original."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Adquirir y visualizar el dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#librerías utilizadas para la adquisición de los datos\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import statsmodels.api as sm\n",
                "import re\n",
                "import seaborn as sns\n",
                "import geopandas as gpd\n",
                "import shapely.wkt\n",
                "from decimal import Decimal\n",
                "import re"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Otros recursos utilizados\n",
                "- dataset de id_geonames: ar_copy.csv\n",
                "- dataset de barrios Argentina: barrios.csv \n",
                "- dataset de municipios Argentina del IGN: municipio.shp"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Leemos y asignamos el dataset properatti.csv en una variable \n",
                "data = pd.read_csv(\"./properatti.csv\", index_col=0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualización de la forma y atributos del dataset \n",
                "print(data.shape)\n",
                "print(\"El dataset está compuesto por:\", data.shape[0], \"filas y\",data.shape[1],\"columnas.\")\n",
                "data.sample(5) #"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Descripción de las columnas del dataset:\n",
                "\n",
                "Los atributos o columas que incluye son:\n",
                "\n",
                "● unmaded: 0: indice de filas\n",
                "\n",
                "● property_type: tipo de inmueble en venta (casa, departamento, ph...)\n",
                "\n",
                "● operation: tipo de operacion inmobiliaria para las propiedades \n",
                "\n",
                "● place_name: ubicacion del inmueble por ciudad/Partido o barrios\n",
                "\n",
                "● place_with_parent_names: ubicacion agrupada del inmueble (Pais|Provincia|Partido o barrio)\n",
                "\n",
                "● country_name: nombre del país donde ocurre la operacion inmobiliaría\n",
                "\n",
                "● state_name: ubicacion del inmueble por provincia\n",
                "\n",
                "● geonames_id: número de identificación en la base de datos GeoNames asociado a la ubicacion por coordenadas\n",
                "\n",
                "● lat-lon: ubicacion de latitud y longitud concatenada\n",
                "\n",
                "● lat  ●lon: ubicacion de latitud y longitud en columnas separadas\n",
                "\n",
                "● price: precio del inmueble\n",
                "\n",
                "● currency: divisa en la que está expresado el precio del inmueble\n",
                "\n",
                "● price_aprox_local_currency: Precio aproximado en la moneda local del país de publicación\n",
                "\n",
                "● surface_total_in_m2: superficie total m² del inmueble\n",
                "\n",
                "● surface_covered_in_m2: Superficie cubierta en m²\n",
                "\n",
                "● price_usd_per_m2: Precio en dolares por metro cuadrado (USD/m²: precio dólares / superficie)\n",
                "\n",
                "● price_per_m2: Precio del metro cuadrado del inmueble\n",
                "\n",
                "● floor: N° de piso (cuando corresponde)\n",
                "\n",
                "● room: cantidad de habitaciones\n",
                "\n",
                "● expenses: expensas (cuando corresponde)\n",
                "\n",
                "● properati_url\t: URL de la inmobiliaría Properati en la Web\n",
                "\n",
                "● description: descripción del inmueble en la publicación Web\n",
                "\n",
                "● title: título del inmueble en la publicación\n",
                "\n",
                "● image_thumbnail: URL de un thumbnail de la primer foto en la Web"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Parsear los datos"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Analisis exploratorio general del dataset Properatti"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Identificamos el tipo de dato de cada columna\n",
                "data.dtypes\n",
                "# El tipo de datos para variables cuantitativas discreta como floor y rooms deberia ser int, \n",
                "# posiblemente tengamos que realizar el cambio en su manipulación."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Realizamos una descripción estadística de todas las columnas (\"include all también muestra las variables categóricas\") \n",
                "# que resume la tendencia central, la dispersión y la forma de la distribución de un conjunto de datos\n",
                "data.describe(include=\"all\")\n",
                "\n",
                "# Algunas interpretaciones/inferencias:\n",
                "# operation y country_name tiene 1 solo dato:  Sell y Argentina, como ya sabíamos, el dataset son datos de venta en Argentina\n",
                "# Existen cuatro tipos de propiedades en venta, la más frecuente es apartamento\n",
                "# Placename tiene como dato más frecuente la ciudad de Cordoba y state_name tiene a Capital Federal\n",
                "# lat-long hay datos repetidos o son los mismos edificios representados en un area determinada\n",
                "# Existen valores máximos muy alejados del resto de datos en las columnas de superficie, floor y rooms, posibles outliers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Identificamos los valores únicos x columna\n",
                "for columnas in data.columns:\n",
                "    print(\"\")\n",
                "    print(f'Nombre:{columnas}')\n",
                "    print(data[columnas].value_counts())\n",
                "\n",
                "# de esta función sumamos información general sobre el data set:\n",
                "# identificamos los tipos de inmueble en venta: apartamentos y casas concentran la mayoría de datos\n",
                "# las divisas más utilizadas son el peso argentina y dólares, hay datos que podemos tomar como no representativos:\n",
                "# el PEN: peso peruano y UYU: peso uruguayo ya que no pasan de dos registros en el dataset. \n",
                "# para floor y rooms hay que tratar los valores outliers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Análisis de datos faltantes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Vemos la cantidad de datos nulos por variable\n",
                "#creamos un nuevo dataframe con la suma de todos los registros nan por columna y el % que representan en forma decreciente\n",
                "missing_data = data.isna().sum(axis=0)\n",
                "missing_data_df = pd.DataFrame(missing_data, columns=['total_nan'])\n",
                "missing_data_df['perc_%'] = (missing_data_df / data.shape[0]).round(2)*100\n",
                "missing_data_df.sort_values(ascending=False, by='total_nan')\n",
                "\n",
                "# price, currency, price_aprox, price_usd tienen la misma cantidad de nulos 20410\n",
                "# hay que averiguar si price_per_m2 es la relación de price y surface_total, así podríamos completar nulos en price_per_m2\n",
                "# podríamos completar place_name por el % bajo de nan\n",
                "# el mayor % de nan se encuentra en floor, rooms y expensas\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analizamos por fila cantidad de datos faltantes\n",
                "missing_by_row = (data.isna().sum(axis=1).value_counts() / data.shape[0] *100).sort_index() \n",
                "missing_by_row"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Lo graficamos\n",
                "sns.barplot(x=missing_by_row.index, y=missing_by_row.values) #grafico de barras\n",
                "plt.title(\"Porcentaje de registros con cantidad de datos faltantes por filas\")\n",
                "plt.xlabel(\"Cantidad de datos faltantes\")\n",
                "plt.ylabel(\"Porcentaje de registros\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Dispersión de datos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vemos el porcentaje de datos unicos con respecto al total. \n",
                "# Nos indica que tan diferentes son los datos en cada variable.  \n",
                "data_dispersion = data.apply(lambda x: x.unique().size)\n",
                "data_dispersion_df = pd.DataFrame(data_dispersion, columns=['count'])\n",
                "data_dispersion_df[\"perc\"] = (data_dispersion / data.shape[0]).round(3)*100\n",
                "data_dispersion_df.sort_values(ascending=True, by='count')\n",
                "#vemos que operation y country_name tienen porcentaje cero ya que tienen un único registro.\n",
                "#el aumento del porcentaje nos indica que tan diferentes son los datos dentro de la variable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#quisimos ver cuales son las variables con datos únicos menores a 100. \n",
                "print(\"Las variables con una cantidad de registros menores a 100 son:\", )\n",
                "for col in data.columns:\n",
                "    if(data[col].nunique() < 100):\n",
                "        print(col)\n",
                "        print(data[col].unique())\n",
                "        print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Analisis de correlacion entre columnas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identificamos si existe una correlacion entre las diferentes variables numericas del dataset\n",
                "corr = data.set_index(\"place_name\").corr()\n",
                "sm.graphics.plot_corr(corr, xnames=list(corr.columns))\n",
                "plt.show()\n",
                "\n",
                "# Hay una alta correlación entre price y price_per_m2, price_aprox_local_currency, price_aprox_usd \n",
                "## hay que ver el tipo de moneda de price seguramente comparta la misma que price_aprox_local_currency\n",
                "# La relación entre price y price_per_m2 se puede deber a que se utiliza price para determinar el precio por m2 del inmueble\n",
                "# la relación entre los variables precios y variables de superficies puede deberse que el precio sea una variable dependiente de la superficie "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exploración del dataset dividido en grupos de Properatti "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Para los siguientes pasos trabajaremos con el dataset dividido en tres grandes grupos:\n",
                "-  Localización: que contiene las columnas relacionadas con la ubicación del inmueble y a su vez este dividido en dos subgrupos:\n",
                "    - Ubicación: que contiene las columnas place_name, state_name, country_name y place_with_parent_names, es decir columnas con la localización política del inmueble.\n",
                "    - Georeferenciada: que contiene las columnas geonames_id, lat y lon, es decir columnas con la localización geográfica del inmueble.\n",
                "- Precio: que contiene las columnas relacionadas a los precios en distintas variantes. \n",
                "- Superficie: que contiene las columnas relacionadas con la superficie del inmueble"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Definimos 4 grupos de variables para poder trabajar con ellas de forma mas sencilla\n",
                "places = ['place_name','place_with_parent_names', 'country_name','state_name']\n",
                "geolocation = ['geonames_id', 'lat-lon','lat','lon']\n",
                "price = ['price','currency','price_aprox_local_currency','price_aprox_usd','price_usd_per_m2','price_per_m2']\n",
                "surface = ['surface_total_in_m2','surface_covered_in_m2']"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Localización por ubicación política"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Exploración de las variables relacionadas con la ubicación: Por Provincia, Ciudad/Barrio, el conjunto de ubicación \n",
                "- country_name\n",
                "- state_name \n",
                "- place_name                        \n",
                "- place_with_parent_names              \n",
                "\n",
                "vamos a:\n",
                "\n",
                "- Explorar las variables \n",
                "- Visualizar los nan de estas columnas\n",
                "- Relacionar las columnas country_name, state_name, place_name con la concatenación de ubicación en place_with_parent_names para ver si efectivamente corresponden o hay datos mal cargados"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Exploramos el conjunto total y las variables \n",
                "# Creamos un nuevo dataframe con las columnas que nos interesan\n",
                "location_data = data[places].copy()\n",
                "location_data.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "places_data = data[places].copy()   #asignamos el dataframe a la variable que utilizaremos en adelante\n",
                "places_data.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "places_data.sample(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Vemos más a detalle por variable los registros totales, los registros únicos, el dato más frecuente y su frecuencia\n",
                "#Vemos que state_name contiene más de las 23 provincias que debería tener Argentina:posiblemente estén datos diferentes a provincias\n",
                "places_data[places].describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#ubicación \"State_name\" ¿por provincia?\n",
                "#vamos a ver un poco más cuales son los valores únicos de state_name que deberían relacionarse al nombre de las provincias\n",
                "places_data[\"state_name\"].value_counts() #Excludes NA values by default.\n",
                "\n",
                "#efectivamente state_name si contiene las 23 provincias argentinas, sin embargo, divide a Buenos Aires en la capital federal\n",
                "# y el gran buenos aires más la costa atlántica \n",
                "# observamos que los datos están concentrados en Buenos Aires (Cap fed, zona norte, zona sur, oeste, atlántica, interior),\n",
                "#  Cordoba, Santa fe "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ubicación por place_name ¿ciudad-municipio o barrio?\n",
                "places_data[\"place_name\"].value_counts().head(20)\n",
                "# NOtamos que existe una mexcla entre ciudades,municipios y barrios.\n",
                "# Tigre es una ciudad al norte de la ciudad de Buenos Aires.\n",
                "# Nordelta es una localidad urbana en el Partido de Tigre, Provincia de Buenos Aires \n",
                "## esta columna mezcla ciudades y barrios\n",
                "# Capital Federal está como place_name "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#nVemos algunos registros de place_with_parent_names\n",
                "places_data[\"place_with_parent_names\"].value_counts().head(10) \n",
                "\n",
                "# La variable representa el país/ \"country_name\", la provincia/ \"state_name\" (o division de la provincia), la ciudad principal/partido-municipio o barrio\n",
                "# en el caso de capital federal\"place_name\"\n",
                "# y un adicional que puede ser una localidad del partido (ejem: |Argentina|Bs.As. G.B.A. Zona Norte|Tigre|*Nordelta*|)\n",
                "# los registros tienen entre 2  y 4 concatenaciones: 2 concatenaciones solo muestra hasta la ubicación por provincia, no brinda información relevante\n",
                "### |Argentina|Capital Federal|  1297 no tiene nombre de barrio, desestimar o completar? \n",
                "### |Argentina|Córdoba|    2648no tiene nombre de barrio, desestimar o completar? \n",
                "# esta variable puede usarse para completar los datos faltantes de place_name \n",
                "# podemos chequear si el atributo \"place_name\" coincide con los datos aquí plasmados"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Para state_name vemos: cuales son los datos de place_name más frecuente, su frecuencia y cuantos registros diferentes existen \n",
                "places_data.groupby([\"state_name\"])[\"place_name\"].describe()\n",
                "\n",
                "# La variable place_name está representado principalmente por una ciudad-municipio de la provincia o division de esta"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Verificar la calidad de los datos:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Ubicación"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Para verificar la calidad de los datos, principalmente, vamos a comparar las columnas que deberían tener la misma información y ver si coinciden o no. Por ejemplo, la columna \"place_with_parent_names\" debería contener dentro de su array la misma información que \"place_name\". Vamos a verificar si esto es cierto o no."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Corroborar si place_with_parent_names coincide con country_name, state_name, place_name "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convertimos la columna place_with_parent_names en una lista usando el separador \"|\"\n",
                "# Eliminamos el separador inicial y final de la lista\n",
                "places_data['place_with_parent_names'] = places_data['place_with_parent_names'].apply(lambda x: x.lstrip(\"|\").rstrip(\"|\").split(\"|\"))\n",
                "places_data['place_with_parent_names'].head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Función para chequear si los valores de la columna place_with_parent_names coinciden exactamente con \n",
                "# los valores de las columnas country_name, state_name, place_name\n",
                "# Coincidir exactamente significa que la columna place_with_parent_names sólo tiene 3 elementos \n",
                "# y que los valores de los elementos coinciden en orden con los valores de las columnas country_name, state_name, place_name\n",
                "\n",
                "def is_location_different(row):\n",
                "    # la lista debería tener 3 elementos\n",
                "    if len(row['place_with_parent_names']) != 3:\n",
                "        return True\n",
                "    if row['country_name'] == row['place_with_parent_names'][0] \\\n",
                "    and row['state_name'] == row['place_with_parent_names'][1] \\\n",
                "    and row['place_name'] == row['place_with_parent_names'][2]:\n",
                "        return False\n",
                "    else:\n",
                "        return True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Creamos la mascara utilizando la función is_location_different\n",
                "mask = places_data.apply(lambda x: is_location_different(x), axis=1)\n",
                "print(\"place_with_parent_names difference with country_name, state_name and place_name:\", places_data[mask].shape[0])\n",
                "places_data[mask][places].sample(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*Conclusión: Existen 45220 registros que no coinciden exactamente.* "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Análisis de los registros que no coinciden exactamente en las columnas places\n",
                "___"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Veamos cuáles son los registros que no coinciden exactamente en las columnas places"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Contamos la cantidad de valores de cada array de place_with_parent_names\n",
                "destructured_places_with_parent_names_rows =  places_data['place_with_parent_names'].apply(lambda x: len(x)).value_counts()\n",
                "destructured_places_with_parent_names_rows\n",
                "# Este código nos devuelve la cantidad de filas que tenemos agrupadas en place_with_parent_names: va de dos a cinco especificaciones\n",
                "# para la ubicación de un inmueble; hasta donde conocíamos, veníamos viendo hasta 3: País|Provincia|Ciudad o Barrio.\n",
                "# veremos que descripción adicional nos brinda las concatenaciones >3."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vemos el porcentaje para graficarlo luego\n",
                "destructured_places_with_parent_names_rows_percent =  places_data['place_with_parent_names'].apply(lambda x: len(x)).value_counts(normalize=True) * 100\n",
                "destructured_places_with_parent_names_rows_percent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Representamos la cantidad de grupo de datos contenidos en la variable place_with_parent_names\n",
                "fig, ax = plt.subplots(figsize=(10,2))\n",
                "sns.barplot(x=destructured_places_with_parent_names_rows_percent.values,y=destructured_places_with_parent_names_rows_percent.index, orient='h', ax=ax, ) \n",
                "ax.set_title('Cantidad de valores de cada array de place_with_parent_names') \n",
                "ax.set_xlabel('Porcentaje sobre el total') \n",
                "ax.set_ylabel('Valores de cada array')\n",
                "plt.xlim(0,100)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "De los pasos anteriores notamos: los registros que no coinciden exactamente son aquellas listas que no tienen 3 valores. De todas maneras tanto aquellos que tiene 2 como los que tienen 5 valores no tienen una cantidad significativa de registros. \n",
                "\n",
                "Revisamos que los valores que están en cada lista tengan su contraparte en las otras columnas aunque figuren en otro orden del array. Por ejemplo, si en place_with_parent_names figura \"Argentina|Capital Federal|Palermo\" y en place_name figura \"Capital Federal\", entonces el registro para este momento del análisis se considera válido."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# country_name\n",
                "print(\"null values:\", places_data['country_name'].isna().sum())\n",
                "mask = ~places_data.apply(lambda x: x['country_name'] in x['place_with_parent_names'], axis=1)\n",
                "print(\"place_with_parent_names difference with country_name:\", places_data[mask].shape[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# state_name\n",
                "print(\"null values:\", places_data['state_name'].isna().sum())\n",
                "mask = ~places_data.apply(lambda x: x['state_name'] in x['place_with_parent_names'], axis=1)\n",
                "print(\"place_with_parent_names difference with state_name:\", places_data[mask].shape[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# place_name\n",
                "print(\"null values:\", places_data['place_name'].isna().sum())\n",
                "mask = ~places_data.apply(lambda x: x['place_name'] in x['place_with_parent_names'], axis=1)\n",
                "print(\"place_with_parent_names difference with place_name:\", places_data[mask].shape[0])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*Conclusión: La información que figura en las columnas place_name, country_name y state_name se encuentra en la columna place_with_parent_names aunque no exactamente igual (parece existir información extra) excepto en la columna 'place_name' con sus 23 NaNs que ya hemos identificado previamente.*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A continuación analizaremos esas columnas dividiendo el trabajo en partes dependiendo de la cantidad de elementos que contenga el array de la columna \"place_with_parent_names\"."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Análisis de registros de place_with_parent_names con 3 valores\n",
                "___"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Revisamos los place_with_parent_names con tres elementos\n",
                "mask = places_data['place_with_parent_names'].apply(lambda x: len(x) == 3)\n",
                "places_data_3_elements = places_data[mask].copy()\n",
                "places_data_3_elements.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Corroboramos que los valores de country_name, state_name y place_name coinciden con los valores de place_with_parent_names\n",
                "print(\"country_name difference with place_with_parent_names[0]:\", places_data_3_elements[places_data_3_elements['country_name'] != places_data_3_elements['place_with_parent_names'].apply(lambda x: x[0])].shape[0])\n",
                "print(\"state_name difference with place_with_parent_names[1]:\", places_data_3_elements[places_data_3_elements['state_name'] != places_data_3_elements['place_with_parent_names'].apply(lambda x: x[1])].shape[0])\n",
                "print(\"place_name difference with place_with_parent_names[2]:\", places_data_3_elements[places_data_3_elements['place_name'] != places_data_3_elements['place_with_parent_names'].apply(lambda x: x[2])].shape[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vemos los datos correspondientes a los valores nulos de la columna place_name\n",
                "mask = ~places_data.apply(lambda x: x['place_name'] in x['place_with_parent_names'], axis=1)\n",
                "places_data_place_name_nan = places_data[mask].copy()\n",
                "places_data_place_name_nan.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Extraemos el dato en la posicion que corresponde a Tigre y lo contabilizamos \n",
                "places_data_place_name_nan['place_with_parent_names'].apply(lambda x: x[2]).value_counts()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*Conclusión: Los registros que no tienen valor en place_name son los que no coinciden, todos del municipio de Tigre. El resto de los valores son exactamente iguales*  \n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Análisis de registros de place_with_parent_names con 4 valores\n",
                "___"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Revisamos los place_with_parent_names con cuatro elementos\n",
                "mask = places_data['place_with_parent_names'].apply(lambda x: len(x) == 4)\n",
                "places_data_4_elements = places_data[mask].copy()\n",
                "places_data_4_elements.shape\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Contamos los valores de la columna place_with_parent_names por el segundo elemento (descartamos el primero\n",
                "# porque sabemos que siempre es Argentina)\n",
                "places_data_4_elements['place_with_parent_names'].apply(lambda x: x[1]).value_counts()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confirmamos que los valores de la columna place_with_parent_names[1] coinciden con los valores de la columna state_name\n",
                "mask = (places_data_4_elements['place_with_parent_names'].apply(lambda x: x[1]) != places_data_4_elements['state_name'])\n",
                "print(\"place_with_parent_names[1] difference with state_name:\", places_data_4_elements[mask].shape[0])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confirmamos que los valores de la columna place_with_parent_names[2] coinciden con los valores de la columna place_name\n",
                "mask = places_data_4_elements['place_with_parent_names'].apply(lambda x: x[2]) != places_data_4_elements['place_name']\n",
                "print(\"place_with_parent_names[2] difference with place_name:\", places_data_4_elements[mask].shape[0])\n",
                "print(\"place_with_parent_names[2] proportional difference with place_name:\", places_data_4_elements[mask].shape[0] / places_data_4_elements.shape[0])\n",
                "places_data_4_elements_non_matching = places_data_4_elements[mask].copy()\n",
                "places_data_4_elements_non_matching[places].sample(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# No coinciden en 29842 registros pero coinciden en el resto. \n",
                "# Confirmamos que los que coinciden son iguales al valor[3] en la cadena completa\n",
                "mask = places_data_4_elements_non_matching['place_with_parent_names'].apply(lambda x: x[2]) == places_data_4_elements_non_matching['place_with_parent_names'].apply(lambda x: x[3])\n",
                "places_data_4_elements_non_matching[mask].shape[0]/ places_data_4_elements_non_matching.shape[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confirmamos que los valores de la columna place_with_parent_names[3] coinciden con los valores de la columna place_name\n",
                "mask = (places_data_4_elements['place_with_parent_names'].apply(lambda x: x[3]) != places_data_4_elements['place_name'])\n",
                "print(\"place_with_parent_names[3] difference with place_name:\", places_data_4_elements[mask].shape[0])\n",
                "anti_mask = ~mask # Los que coinciden\n",
                "places_data_4_elements[anti_mask][places].sample(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*Conclusión: De los 39869 registros con 4 valores, 29842 toman el valor de place_name del 4to valor de la lista. Los 10027 restantes toman el 3er valor.* que es siempre un barrio/localidad dentro del municipio: ejm: Glew localidad del municipio Almiramte Brown de zona sur del Gran Buenos Aires.\n",
                "\n",
                "- place_name asignado a 3er valor de \"place_with_parent_names\": 10027\n",
                "- place_name asignado a 4to valor de \"place_with_parent_names\": 29842"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Análisis de registros de place_with_parent_names con 2 valores\n",
                "___"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reviso los place_with_parent_names con dos elementos\n",
                "mask = places_data['place_with_parent_names'].apply(lambda x: len(x) == 2)\n",
                "places_data_2_elements = places_data[mask].copy()\n",
                "places_data_2_elements['place_with_parent_names'].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reviso los place_with_parent_names de Córdoba y Capital Federal por contener el maypr numero de registros\n",
                "mask = places_data_2_elements['place_with_parent_names'].apply(lambda x: x[1] == 'Córdoba' or x[1] == 'Capital Federal')\n",
                "places_data_2_elements[mask].sample(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pareciera ser que todos los lugares que tienen dos elementos en place_with_parent_names repiten el nombre del estado en place_name\n",
                "# Corroboramos esto\n",
                "mask = places_data_2_elements['place_name'] != places_data_2_elements['state_name']\n",
                "places_data_2_elements[mask].shape\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*Conclusión: En los 4780 registros de place_with_parent_names que tienen 2 valores se utilizó state_name como place_name.* \n",
                "*Es decir, la ciudad principal de la provincia.*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Análisis de registros de place_with_parent_names con 5 valores\n",
                "___"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Revisamos los place_with_parent_names con 5 elementos\n",
                "mask = places_data['place_with_parent_names'].apply(lambda x: len(x) == 5)\n",
                "places_data_5_elements = places_data[mask].copy()\n",
                "print(places_data_5_elements['place_with_parent_names'].shape)\n",
                "places_data_5_elements.sample(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Revisamos si alguno de los valores no corresponde a Nordelta\n",
                "places_data_5_elements['place_with_parent_names'].apply(lambda x: x[3] != 'Nordelta').sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Corroboramos que los valores de la columna place_with_parent_names[2] coinciden con los valores de la columna place_name\n",
                "mask = (places_data_5_elements['place_with_parent_names'].apply(lambda x: x[2]) != places_data_5_elements['place_name'])\n",
                "print(\"place_with_parent_names[2] difference with place_name:\", places_data_5_elements[mask].shape[0])\n",
                "print(\"place_with_parent_names[2] proportional difference with place_name:\", places_data_5_elements[mask].shape[0] / places_data_5_elements.shape[0])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Corroboramos que los valores de la columna place_with_parent_names[3] coinciden con los valores de la columna place_name\n",
                "mask = (places_data_5_elements['place_with_parent_names'].apply(lambda x: x[3]) != places_data_5_elements['place_name'])\n",
                "print(\"place_with_parent_names[3] difference with place_name:\", places_data_5_elements[mask].shape[0])\n",
                "print(\"place_with_parent_names[3] proportional difference with place_name:\", places_data_5_elements[mask].shape[0] / places_data_5_elements.shape[0])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Corroboramos que los valores de la columna place_with_parent_names[4] coinciden con los valores de la columna place_name\n",
                "mask = (places_data_5_elements['place_with_parent_names'].apply(lambda x: x[4]) != places_data_5_elements['place_name'])\n",
                "print(\"place_with_parent_names[4] difference with place_name:\", places_data_5_elements[mask].shape[0])\n",
                "print(\"place_with_parent_names[4] proportional difference with place_name:\", places_data_5_elements[mask].shape[0] / places_data_5_elements.shape[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*Conclusión: Los 548 registros de place_with_parent_names que tienen 5 valores son de Nordelta y el último valor se refiere al Barrio. place_name toma los valores del barrio*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "> ##### Conclusiones finales: \n",
                "> - Existen como máximo 5 tipos de registros en place_with_parent_names: País, Provincia, Municipio, Ciudad, Barrio\n",
                "> - País y Provincia son iguales a country_name y state_name\n",
                "> - Place_name se le asigna el valor de municipio: 76000 + 10027 = 86027\n",
                "> - Place_name se le asigna el valor de ciudad: 29842 (en  estos casos existe el valor municipio también )\n",
                "> - Place_name se le asigna el valor de barrio: 548 (en estos casos existe el valor municipio y ciudad también)\n",
                "> - Place_name se le asigna el valor de state_name: 4780 (en estos casos no existe el valor municipio y ciudad)\n",
                "> - Place_name se le asigna el valor de NaN: 23 (en estos casos existe el valor municipio y ciudad)\n",
                "> ___\n",
                "> Por lo tanto se puede concluir que para dar mayor consistencia es posible imputar place_name con el valor del municipio en las mayoría de los casos. Confrontar con geoNamesId.\n",
                "> ___\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Geolocalización"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Creamos un nuevo dataframe con las columnas que nos interesan\n",
                "geo_location_data = data[ geolocation].copy()\n",
                "geo_location_data.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Analizamos  lat-lon para ver si habían registros duplicados considerando que si existe una misma coordenadas son las misma propiedad."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Creamos una copia de data en la que podamos borrar los nan y agrupamos por lat y lon, contamos sus valores y\n",
                "# le decimos que nos devuelva cuantos registros hay mayores a 1 en forma descendente para todo el dataset.\n",
                "data_copy = data.copy()\n",
                "data_copy.dropna(subset=['lat-lon'], inplace=True)\n",
                "data_copy_group = data_copy.groupby('lat-lon').count()\n",
                "data_copy_group[data_copy_group['place_name'] > 1].sort_values(by='operation', ascending=False)\n",
                "\n",
                "#llama la atención que existan coordenadas repetidas, podríamos inferir que las coordenadas no son especificas de la ubicación del\n",
                "# inmueble, sino de un punto de referencia  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## seguimos chequeando si si existe algún dato duplicado en lat-lon\n",
                "print(data.duplicated().any())\n",
                "print(data['lat-lon'].duplicated().any())\n",
                "print(data_copy.shape)\n",
                "\n",
                "# no existen columnas duplicadas, pero si existen datos de lat-lon iguales en un total de 69670 filas "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Quisimos ver los datos completos que tienen como coordenadas los 312 casos que vimos anteriormente\n",
                "data_copy[data_copy['lat-lon'] == '-34.4026444,-58.6684776']\n",
                "\n",
                "#vemos que los datos repetidos de lat y lon no se deben al mismo inmueble"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "geo_location_data.sample(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "No es posible determinar si es la misma propiedad porque lat-lon se refiere muchas veces a la ubicación aproximada. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vemos el porcentaje de valores nulos por columna\n",
                "geo_location_data.isnull().sum()/geo_location_data.shape[0] * 100\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualizamos en el mapa de SurAmerica si los datos de lat y lon pertenecen efectivamente a Argentina\n",
                "# convertimos el dataframe a geodataframe\n",
                "geo_location_data_gdf = gpd.GeoDataFrame(geo_location_data, geometry=gpd.points_from_xy(geo_location_data.lon, geo_location_data.lat))\n",
                "\n",
                "# Ubicamos los puntos en el mapa\n",
                "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
                "latin_america = world[world['continent'] == 'South America']\n",
                "fig, ax = plt.subplots(figsize=(5,10))\n",
                "latin_america.plot(ax=ax, alpha=0.4, color='grey', edgecolor='black')\n",
                "geo_location_data_gdf.plot(ax=ax, markersize=0.5, color='red')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Existe una propiedad que se encuentra fuera de la Argentina (en Colombia). \n",
                "# La buscamos en el dataframe original\n",
                "# buscamos el polígono de Colombia\n",
                "geo_colombia = world[world['name'] == 'Colombia']\n",
                "\n",
                "# Buscamos la propiedad que se encuentra en Colombia\n",
                "geo_outlier_index = geo_location_data_gdf[geo_location_data_gdf.within(geo_colombia['geometry'].iloc[0])].index\n",
                "\n",
                "# La buscamos en el dataframe original\n",
                "data.iloc[geo_outlier_index]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Es una propiedad súper interesante, por latitud y longitud está en Colombia, pero por el nombre de la provincia está en Argentina, la descripción habla de Armenia y el título del el Barrio el Limonar.\n",
                "\n",
                "En una rápida búsqueda por internet encontramos que la propiedad se encuentra en el barrio El Limonar de Armenia, Colombia.\n",
                "\n",
                "La descartaremos sin dudas.\n",
                "\n",
                "______"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Corroborar si lat-lon coincide con las columnas lat y lon"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Corroboremos que los valores nulos en lat-lon son los mismos que en lat y lon\n",
                "geo_location_data[geo_location_data['lat-lon'].isnull()]['lat'].isnull().sum() == geo_location_data[geo_location_data['lat-lon'].isnull()]['lon'].isnull().sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dropeamos los valores nulos de lat-lon\n",
                "geo_location_data.dropna(subset=['lat-lon'], inplace=True)\n",
                "geo_location_data.isnull().sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convertimos la columna lat-lon en dos columnas nuevas\n",
                "geo_location_data['lat_alt'] = geo_location_data['lat-lon'].apply(lambda x: x.split(',')[0])\n",
                "geo_location_data['lon_alt'] = geo_location_data['lat-lon'].apply(lambda x: x.split(',')[1])\n",
                "geo_location_data.head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vemos los tipos de datos\n",
                "geo_location_data.dtypes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# convertimos las nuevas columnas a float\n",
                "geo_location_data['lat_alt'] = geo_location_data['lat_alt'].astype(float)\n",
                "geo_location_data['lon_alt'] = geo_location_data['lon_alt'].astype(float)\n",
                "geo_location_data.dtypes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comprobamos que los valores de las nuevas columnas son iguales a los de las columnas originales\n",
                "print(\"Diferencia entre lat:\",(geo_location_data['lat_alt'] != geo_location_data['lat']).sum())\n",
                "print(\"Diferencia entre lon:\",(geo_location_data['lon_alt'] != geo_location_data['lon']).sum())\n",
                "# En proporción\n",
                "print(\"Diferencia en proporción de lat\",(geo_location_data['lat_alt'] != geo_location_data['lat']).sum()/ geo_location_data.shape[0])\n",
                "print(\"Diferencia en proporción de lon\",(geo_location_data['lon_alt'] != geo_location_data['lon']).sum()/ geo_location_data.shape[0])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Hay diferencias en 21% los casos. Veamos si es una diferencia significativa\n",
                "\n",
                "Revisamos el margen de diferencia redondeando progresivamente."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Decimal(geo_location_data['lon'][0]).as_tuple().exponent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reviso la cantidad de decimales que tienen los valores de lat y lon\n",
                "geo_location_data['lat'].apply(lambda x: Decimal(x).as_tuple().exponent).value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "geo_location_data['lon'].apply(lambda x: Decimal(x).as_tuple().exponent).value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vamos redondeando progresivamente los valores de lat y lon\n",
                "lat_decimal_diff = []\n",
                "for i in range(47,0,-1):\n",
                "    margin = (geo_location_data['lat_alt'].round(i) != geo_location_data['lat'].round(i)).sum()/ geo_location_data.shape[0]\n",
                "    lat_decimal_diff.insert(0,margin)\n",
                "    # print('Margen de diferencia de',i,'decimales en latitud:', margin)\n",
                "# Buscamos dónde el margen de diferencia es menor al 1%\n",
                "    if margin <= 0.01:\n",
                "        print('Margen de diferencia de',i,'decimales en latitud:', margin)\n",
                "   \n",
                "print(\"////////////////////////////////////\")   \n",
                "\n",
                "lon_decimal_diff = []\n",
                "for i in range(47,0,-1):\n",
                "    margin = (geo_location_data['lon_alt'].round(i) != geo_location_data['lon'].round(i)).sum()/ geo_location_data.shape[0]\n",
                "    lon_decimal_diff.insert(0,margin)\n",
                "    # print('Margen de diferencia de',i,'decimales en longitud:', margin)\n",
                "# Buscamos dónde el margen de diferencia es menor al 1%\n",
                "    if margin <= 0.01:\n",
                "        print('Margen de diferencia de',i,'decimales en latitud:', margin)\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Lo graficamos\n",
                "plt.figure(figsize=(10,5))\n",
                "plt.plot(lat_decimal_diff, label='latitud')\n",
                "plt.plot(lon_decimal_diff, label='longitud')\n",
                "plt.ylabel('Margen de diferencia')\n",
                "plt.xlabel('Cantidad de decimales')\n",
                "plt.legend()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "geo_location_data_gdf = gpd.GeoDataFrame(geo_location_data, geometry=gpd.points_from_xy(geo_location_data.lon, geo_location_data.lat))\n",
                "geo_location_data_gdf.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "> Como se puede ver, las diferencias entre lat-lon y lat y lon comienzan a partir de los 13 decimales por lo que podemos descartarla como una diferencia significativa.\n",
                "> \n",
                "> Por otro lado, también hemos comprobado que los valores de lat lon provienen de convertir la variable a geometry y obtener de ahí lat y lon\n",
                ">\n",
                "> Entonces podemos concluir que la variable lat-lon es redundante y podemos eliminarla.\n",
                "> ____ "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Análisis de propiedades con igual latitud y longitud con otras propiedades"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Vamos a buscar si hay muchos puntos iguales para refutar la hipótesis de que se utilizó la misma ubicación para varias propiedades"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Primero veamos una visualización de los datos de Capital Federal para ver si parece haber una gran concentración \n",
                "# de propiedades en algún lugar específico o están dispersas por toda la ciudad.\n",
                "geo_location_data_gdf_capital = geo_location_data_gdf[data['state_name']=='Capital Federal' ]\n",
                "geo_location_data_gdf_capital['geometry']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ubicamos los puntos en el mapa\n",
                "df_barrios_capital = pd.read_csv('./data/barrios.csv', sep=',', encoding='latin-1')\n",
                "import shapely.wkt\n",
                "\n",
                "df_barrios_capital[\"WKT\"] = df_barrios_capital[\"WKT\"].apply(shapely.wkt.loads) \n",
                "df_barrios_capital = gpd.GeoDataFrame(df_barrios_capital, geometry='WKT')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(10,20))\n",
                "\n",
                "geo_location_data_gdf_capital.plot(ax=ax, markersize=10, color='red', alpha=0.1) \n",
                "df_barrios_capital.plot(ax=ax, alpha=0.4, color='grey', edgecolor='black')\n",
                "plt.xlim(-58.55,-58.350)\n",
                "plt.ylim(-34.705,-34.525) \n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ahora veamos en todo el dataset cuántas propiedades comparten la misma ubicación.\n",
                "different_locations = geo_location_data_gdf['geometry'].value_counts()\n",
                "print('Cantidad de propiedades por punto:', different_locations) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prop_qty_per_point = []\n",
                "for i in range(1,different_locations[0]+1):\n",
                "    (different_locations == i).sum() / different_locations.shape[0] * 100\n",
                "    prop_qty_per_point.append((different_locations == i).sum() / different_locations.shape[0] * 100)\n",
                "prop_qty_per_point = pd.DataFrame(prop_qty_per_point, columns=['Porcentaje'], index=range(1,different_locations[0]+1))\n",
                "prop_qty_per_point.reset_index(inplace=True)\n",
                "prop_qty_per_point.rename(columns={'index':'Cantidad de propiedades por punto'}, inplace=True)\n",
                "prop_qty_per_point.head(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Lo graficamos\n",
                "plt.figure(figsize=(10,5))\n",
                "plt.hist(different_locations, bins=different_locations[0]+1)\n",
                "plt.ylabel('Cantidad de puntos')\n",
                "plt.xlabel('Cantidad de propiedades por punto')\n",
                "plt.show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Veamos dónde se encuentran las propiedades que más tienen lat y lon iguales"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "most_shared_point_props = geo_location_data_gdf[geo_location_data_gdf['geometry'].isin(different_locations.index[0:11])]\n",
                " "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(5,10))\n",
                "latin_america.plot(ax=ax, alpha=0.4, color='grey', edgecolor='black')\n",
                "most_shared_point_props.plot(ax=ax, markersize=0.5, color='red')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Parece que están todas en la Argentina\n",
                "# Descargamos un mapa de la Argentina por municipios para averiguar qué municipios son los que tienen más propiedades en el mismo punto.\n",
                "# https://www.ign.gob.ar/NuestrasActividades/InformacionGeoespacial/CapasSIG\n",
                "municipios_geo = gpd.read_file('./data/municipio/municipio.shp')\n",
                "municipios_geo.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(5,10))\n",
                "municipios_geo.plot(ax=ax, alpha=0.4, color='grey', edgecolor='black')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Veamos en qué municipios están las propiedades que comparten el mismo punto.\n",
                "# convertimos a coordenadas geográficas para poder hacer el join con el geodataframe de las propiedades.\n",
                "most_shared_point_props.crs = \"EPSG:4326\"\n",
                "most_shared_point_props = most_shared_point_props.to_crs(epsg=4326) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hacemos el join con el geodataframe de los municipios.\n",
                "most_shared_point_props_municipios = gpd.sjoin(most_shared_point_props, municipios_geo, how=\"inner\")\n",
                "most_shared_point_props_municipios.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "most_shared_point_props_municipios['fna'].value_counts()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> Podemos concluir que la gran mayoría de los puntos de lat y lon son únicos, por lo que descartamos que sean aproximaciones.\n",
                ">  \n",
                "> Una excepción notable son Tigre y Capital Federal\n",
                "> \n",
                "> ____"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Corroborar si lat-lon coincide con geonames_id"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#creamos una copia de las variables de geolocación\n",
                "geo_location_data = data[ geolocation].copy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Corroboramos que todos los nulos de latitud y longitud también son nulos en la columna geonames_id.\n",
                "lat_nulls = geo_location_data[geo_location_data['lat'].isnull()]\n",
                "lon_nulls = geo_location_data[geo_location_data['lon'].isnull()]\n",
                "geonames_null = geo_location_data[geo_location_data['geonames_id'].isnull()]\n",
                "print('Es lat null = lon null?: ', lat_nulls.equals(lon_nulls))  \n",
                "print('Es lat null = geonames null?: ', lat_nulls.equals(geonames_null))  \n",
                "print('Es lon null = geonames null?: ', lon_nulls.equals(geonames_null))  \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vemos cuantas columnas tienen nulos en lat (o lon, son iguales en cuanto a nulos) y no en geonames_id.\n",
                "lat_nulls = geo_location_data[geo_location_data['lat'].isnull()]\n",
                "lat_nulls_geonames_not_null = lat_nulls[lat_nulls['geonames_id'].notnull()]\n",
                "lat_nulls_geonames_not_null.shape[0] "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vemos cuantas columnas tienen nulos en geonames_id y no en lat (o lon, son iguales en cuanto a nulos).\n",
                "geonames_null = geo_location_data[geo_location_data['geonames_id'].isnull()]\n",
                "geonames_null_lat_not_null = geonames_null[geonames_null['lat'].notnull()]\n",
                "geonames_null_lat_not_null.shape[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> Conclusión: Podemos imputar lat y lon a partir de geonames_id en 43365 casos. \n",
                "> \n",
                "> Podríamos imputar 10532 casos en geonames pero con una sóla variable de geolocalización es suficiente y usaremos lat-lon para crear geometry.\n",
                ">  \n",
                "> ______"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### Verificamos si es posible importar la información de lat y lon desde geonames_id oficial "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# leamos y cargamos el dataset con los geonames id de Argentina\n",
                "geonames = pd.read_csv(\"./data/ar_copy.csv\", sep='\\t', header=None)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# como no hay nombres en la columnas, para mejorar la extracción renombro las que me interesan\n",
                "geonames.rename({0: 'geoname_oficial', 4:\"lat_oficial\", 5:\"lon_oficial\"}, axis=1, inplace=True)\n",
                "geonames.head(4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ahora buscamos los datos de lat y lon desde el geoname_oficial usando como clave el id geoname (geoname_oficial) y el dato de\n",
                "# latitud y longitud asociada\n",
                "\n",
                "#Creamos un diccionario vacío para ubicar la Latitud\n",
                "lat_dict = {}\n",
                "#Creamos una tupla con los pares de key y value: usando un iterador de tuplas zip donde el primer\n",
                "# elemento de cada iterador pasado se empareja con el primero del segundo y asi sucesivamente\n",
                "geoname_lat = zip(geonames['geoname_oficial'], geonames['lat_oficial'])\n",
                "\n",
                "#completamos el diccionario con un for\n",
                "for geoname, lat_oficial in geoname_lat:\n",
                "    lat_dict[geoname] = lat_oficial"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Creamos un diccionario para ubicar la Longitud\n",
                "lon_dict = {}\n",
                "\n",
                "#Creamos una tupla con los pares de key y value: usando un iterador de tuplas zip donde el primer\n",
                "# elemento de cada iterador pasado se empareja con el primero del segundo y asi sucesivamente\n",
                "geoname_lon = zip(geonames['geoname_oficial'], geonames['lon_oficial'])\n",
                "\n",
                "#completamos el diccionario\n",
                "for geoname, lon_oficial in geoname_lon:\n",
                "    lon_dict[geoname] = lon_oficial"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Agregamos dos nuevas columnas a nuestro dataset de geo_location con lat y lon mapeados del diccionario, para lo cual\n",
                "#usando el geoname_id de nuestro dataset le indicamos que mapee si es el mismo entonces extraiga el valor de lat y lon\n",
                "# a las nuevas columnas\n",
                "geo_location_data[\"lat_geoname\"] = geo_location_data['geonames_id'].map(lat_dict)\n",
                "geo_location_data[\"lon_geoname\"] = geo_location_data['geonames_id'].map(lon_dict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Veamos cuantos valores con geonames_id no nulos no tienen latitud y longitud.\n",
                "missing_georeference_from_geonames = geo_location_data[geo_location_data['lat_geoname'].isnull()]\n",
                "missing_georeference_from_geonames.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filtramos los que tienen geonames_id pero no latitud y longitud heredada por geonames.\n",
                "missing_geo_names_location_data = geo_location_data[geo_location_data['lat_geoname'].isnull() & geo_location_data['geonames_id'].notnull()]\n",
                "print(missing_geo_names_location_data.shape)\n",
                "missing_geo_names_location_data.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "missing_geo_names_location_data['geonames_id'].value_counts() # Es Palermo, Capital Federal"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*Los registros con Geonames_id 3435548 (que corresponden a Palermo) ya tienen lat y lon por lo no nos importa el lat y lon de geonames*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# vemos la cantidad de registros nulos del data geo_location con los nuevos datos de lat y lon agregados\n",
                "print(geo_location_data.isna().sum())\n",
                "# Nos damos cuenta que siguen existiendo registros de latitud y longitud que no se pudieron mapear del dataset con id_geonames oficial."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "geo_location_data[geo_location_data['lat'].isnull() & geo_location_data['lat_geoname'].notnull()]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#si usaramos los datos del dataset ar_copy pudieramos imputar el 37,19% de los datos faltantes de lat y lon en nuestro dataset\n",
                "(geo_location_data[\"lat_geoname\"].isna().sum() / geo_location_data[\"lat\"].isna().sum() *100).round(2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### Comparar lat-lon con lat-lon extraído de geonames_id"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Esto no se puede hacer porque sólo se extrajeron los datos de geonames_id que no tenían lat-lon"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# comparo los datos completados con los datos originales para ver si hay diferencias\n",
                "# remuevo los nan de los datos originales (porque seguro va a haber diferencias en esos datos)\n",
                "geo_location_data_geonames_vs_latlon = geo_location_data.dropna(subset=['lat', 'lon','geonames_id', 'lat_geoname', \"lon_geoname\" ])\n",
                "geo_location_data_geonames_vs_latlon.isna().sum()  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "geo_location_data_geonames_vs_latlon.shape "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# comparamos las filas en común que no tienen nulos\n",
                "geo_location_data_geonames_vs_latlon['lat'] == geo_location_data_geonames_vs_latlon['lat_geoname']\n",
                "geo_location_data_geonames_vs_latlon[geo_location_data_geonames_vs_latlon['lat'] != geo_location_data_geonames_vs_latlon['lat_geoname']]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ubicamos los puntos en el mapa\n",
                "\n",
                "# Estos df ya fueron creado y están puestos como referencia\n",
                "# geo_location_data_gdf = gpd.GeoDataFrame(geo_location_data, geometry=gpd.points_from_xy(geo_location_data.lon, geo_location_data.lat))\n",
                "# geo_location_data_gdf_capital = geo_location_data_gdf[data['state_name']=='Capital Federal']\n",
                "\n",
                "\n",
                "geo_location_data_gdf_geonames = gpd.GeoDataFrame(geo_location_data_geonames_vs_latlon, geometry=gpd.points_from_xy(geo_location_data_geonames_vs_latlon.lon_geoname, geo_location_data_geonames_vs_latlon.lat_geoname))\n",
                "df_barrios_capital = pd.read_csv('./data/barrios.csv', sep=',', encoding='latin-1')\n",
                "\n",
                "\n",
                "df_barrios_capital[\"WKT\"] = df_barrios_capital[\"WKT\"].apply(shapely.wkt.loads) \n",
                "df_barrios_capital = gpd.GeoDataFrame(df_barrios_capital, geometry='WKT')\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10,20))\n",
                "\n",
                "geo_location_data_gdf_capital.plot(ax=ax, markersize=0.5, color='red', alpha=0.1) \n",
                "geo_location_data_gdf_geonames.plot(ax=ax, markersize=10, color='blue', alpha=0.1)\n",
                "df_barrios_capital.plot(ax=ax, alpha=0.4, color='grey', edgecolor='black')\n",
                "plt.xlim(-58.55,-58.350)\n",
                "plt.ylim(-34.705,-34.525) \n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*Conclusión: Geonames_id marca el centro de cada uno de los barrios y no la latitud y longitud exacta de la propiedad. Por lo que deberíamos imputar lat y lon a partir de geonames_id con mucho cuidado.*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Corroborar si lat-lon coincide con place_name"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "municipios_geo.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "municipios_geo_nam_geo = municipios_geo.copy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reemplazamos ciudades por municipios\n",
                "places_data_4_elements_with_mun_as_place_name = places_data_4_elements.copy()\n",
                "places_data_4_elements_with_mun_as_place_name['place_name'] = places_data_4_elements['place_with_parent_names'].apply(lambda x: x[2])\n",
                "print(places_data_4_elements_with_mun_as_place_name.shape)\n",
                "places_data_4_elements_with_mun_as_place_name.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Unimos los nuevos municipios asignados con los otro municipios\n",
                "places_data_mun = pd.concat([places_data_4_elements_with_mun_as_place_name, places_data_3_elements], axis=0) \n",
                "print(places_data_mun.shape) \n",
                "places_data_mun.head() "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Unimos los municipios con su georeferencia\n",
                "places_data_mun_geo = places_data_mun.join(geo_location_data)\n",
                "print(places_data_mun_geo.shape)\n",
                "places_data_mun_geo.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Lo convertimos en geo data frame\n",
                "places_data_mun_geo_gdf = gpd.GeoDataFrame(places_data_mun_geo, geometry=gpd.points_from_xy(places_data_mun_geo.lon, places_data_mun_geo.lat))\n",
                "places_data_mun_geo_gdf.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# limpiamos los datos que no tienen georeferencia\n",
                "places_data_mun_geo_gdf.dropna(subset=['lat', 'lon'], inplace=True)\n",
                "places_data_mun_geo_gdf = places_data_mun_geo_gdf[['place_name', 'geometry','state_name']]\n",
                "print(places_data_mun_geo_gdf.shape)\n",
                "places_data_mun_geo_gdf.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# convertimos a coordenadas geográficas para poder hacer el join con el geodataframe de las propiedades.\n",
                "places_data_mun_geo_gdf.crs = \"EPSG:4326\" \n",
                "places_data_mun_geo_gdf = places_data_mun_geo_gdf.to_crs(\"EPSG:4326\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# unimos los municipios con su georeferencia con los municipios con su georeferencia de places_data_mun_geo_gdf\n",
                "common_municipios = gpd.sjoin(municipios_geo_nam_geo, places_data_mun_geo_gdf, how=\"inner\")\n",
                "common_municipios = common_municipios[['nam','fna', 'place_name','state_name', 'geometry']]\n",
                "print(common_municipios.shape)\n",
                "common_municipios.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chequeamos que nam y place_name sean iguales\n",
                "(common_municipios['nam'] == common_municipios['place_name']).sum() == common_municipios.shape[0] \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vemos que municipios que coinciden con place_name\n",
                "common_municipios[common_municipios['nam'] == common_municipios['place_name']]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vemos que municipios que no coinciden con place_name\n",
                "common_municipios[common_municipios['nam'] != common_municipios['place_name']]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Considerando que el patrón Comuna n es de Capital Federal, eliminamos las filas que tenga ese patrón y el state_name sea Capital Federal\n",
                "regex_comuna = r'Comuna \\d+'\n",
                "common_municipios = common_municipios[ ~((common_municipios['nam'].str.contains(regex_comuna)) & (common_municipios['state_name'] == 'Capital Federal'))]\n",
                "print(common_municipios.shape)\n",
                "uncommon_municipios = common_municipios[common_municipios['nam'] != common_municipios['place_name']]\n",
                "print(uncommon_municipios.shape)\n",
                "uncommon_municipios.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Municipios diferentes: \", uncommon_municipios['nam'].nunique())\n",
                "uncommon_municipios['place_name'].value_counts().head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "uncommon_municipios_mar_del_plata = uncommon_municipios[uncommon_municipios['place_name'] == 'Mar del Plata'] \n",
                "print(uncommon_municipios_mar_del_plata.shape) \n",
                "uncommon_municipios_mar_del_plata.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mar del Plata corresponde al municipio de General Pueyrredón, por lo que también lo eliminamos.\n",
                "common_municipios = common_municipios[ ~((common_municipios['nam'] == 'General Pueyrredón') & (common_municipios['place_name'] == 'Mar del Plata'))]\n",
                "print(common_municipios.shape)\n",
                "uncommon_municipios = common_municipios[common_municipios['nam'] != common_municipios['place_name']]\n",
                "print(uncommon_municipios.shape)\n",
                "uncommon_municipios.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "uncommon_municipios.groupby([ 'nam','place_name']).count().sort_values(by='state_name', ascending=False).head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*Conclusión: Los datos de lat-lon coinciden con place_name.*\n",
                "\n",
                "Casos notables: \n",
                "- Capital Federal: según el df del IGN son comunas y según el original son barrios. Se asume que son correctos.\n",
                "- Mar del Plata: Mar del Plata es la ciudad y General Pueyrredón el municipio. Si se utiliza el municipio como place_name lo correcto es imputarlo. # https://www.argentina.gob.ar/buenosaires/municipios"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Analizar si es posible imputar place_name de los registros deplaces_data_2_elements(donde solo hay país y provincia)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Unimos los municipios con su georeferencia\n",
                "places_data_2_elements_geo = places_data_2_elements.join(geo_location_data)  \n",
                "print(places_data_2_elements_geo.shape)\n",
                "places_data_2_elements_geo.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vemos si existe geonames_id en alguna fila en places_data_2_elements_geo\n",
                "places_data_2_elements_geo['geonames_id'].isnull().sum() == places_data_2_elements_geo.shape[0]  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vemos si existe lat y lon en alguna fila en places_data_2_elements_geo\n",
                "places_data_2_elements_geo['lat'].isnull().sum() == places_data_2_elements_geo.shape[0]  and places_data_2_elements_geo['lon'].isnull().sum() == places_data_2_elements_geo.shape[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*Conclusión: No es posible imputar place_name en los registros donde place name es una provincia a partir de su georeferencia*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Superficie"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 1- Recuperación de informacion faltante en la variable de referencia aplicando regex a la columna descripción"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#se aplica regex a la columna descripcion, buscando las palabras metros, mts y m2 precedido de digitos. \n",
                "serie_descripcion = data_limpia[\"description\"]\n",
                "\n",
                "pattern_sup_m2 = \"(?P<sup>\\d{2,}(,|.\\d+)?)\\s*(m2|metros|mts)\"\n",
                "pattern_sup_m2_regex = re.compile(pattern_sup_m2,  re.IGNORECASE)\n",
                "\n",
                "sup_match = serie_descripcion.apply(lambda x: x if x is np.NaN else pattern_sup_m2_regex.search(x))\n",
                "mask_sup_match_notnull= sup_match.notnull()\n",
                "\n",
                "data_limpia.loc[mask_sup_match_notnull, \"Sup_m2_Clean\"] = sup_match[mask_sup_match_notnull].apply(lambda x: x.group(\"sup\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Al analizar la información se detecta errores de formato, no posibles de salvaguardar usando los metodos replace o to_numeric, por lo cual se decide aplicar nuevamente regex para sacar la partes enteras de los numeros, lo cual nos permitira poder usar esta informacion en estaditicas mas adelante.\n",
                "serie_sup_m2_clean=data_limpia[\"Sup_m2_Clean\"]\n",
                "\n",
                "pattern_sup_m2_dos=\"(?P<supdos>\\d{2,})\"\n",
                "pattern_sup_m2_regex_dos=re.compile(pattern_sup_m2_dos)\n",
                "sup_match_dos = serie_sup_m2_clean.apply(lambda x: x if x is np.NaN else pattern_sup_m2_regex_dos.search(x))\n",
                "mask_sup_match_notnull_dos= sup_match_dos.notnull()\n",
                "data_limpia.loc[mask_sup_match_notnull_dos, \"Sup_m2_Clean_dos\"] = sup_match_dos[mask_sup_match_notnull_dos].apply(lambda x: x.group(\"supdos\"))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#se verifica información obtenida con regex\n",
                "data_limpia[[\"surface_total_in_m2\", \"surface_covered_in_m2\", \"Sup_m2_Clean_dos\"]].sample(20)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#se revisan datos que son mayores a 10000 m2 y que impactarian en el modelo al reemplazar la superficie total.\n",
                "data_limpia[\"Sup_m2_Clean_dos\"]=pd.to_numeric(data_limpia[\"Sup_m2_Clean_dos\"])\n",
                "mask_consistencia_regex= (data_limpia[\"Sup_m2_Clean_dos\"] > 10000) & (data_limpia[\"surface_total_in_m2\"].isnull())\n",
                "consistencia_regex=data_limpia[mask_consistencia_regex]\n",
                "consistencia_regex[[\"surface_total_in_m2\", \"surface_covered_in_m2\", \"Sup_m2_Clean_dos\"]]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "consistencia_regex.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#se decide eliminar los valores analizados por ser estos incoherentes, e impactarian como outliers en los analisis posteriores.\n",
                "data_limpia[\"Sup_m2_Clean_tres\"]=data_limpia[\"Sup_m2_Clean_dos\"].copy()\n",
                "\n",
                "data_limpia.loc[mask_consistencia_regex,\"Sup_m2_Clean_tres\"] = 0"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Durante la imputación se podra corroborar el aporte de la información recuperada"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 2- Revisión la coherencia entre las columnas Superficie total y superficie cubierta."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#se verifican valores de superficie total vs superficie cubierta, entendiendo que la primera siempre debe ser mayor que la segunda.\n",
                "mask_consistencia_sup = data_limpia[\"surface_total_in_m2\"] < data_limpia[\"surface_covered_in_m2\"]\n",
                "data_limpia[mask_consistencia_sup].shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se detectan 1106 valores cuya superficie total es menor que la superficie cubierta, siendo esto inconsistente."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se revisa el contenido de las columnas comparandolas entre si, ademas se la compara con la informacion obtenida en descripcion para comprobar coherencia de los datos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_consistencia_sup = data_limpia[mask_consistencia_sup]\n",
                "data_consistencia_sup[[\"surface_total_in_m2\",\"surface_covered_in_m2\",\"Sup_m2_Clean_dos\", \"property_type\"]].sample(20)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#se analiza la consistencia de los datos para valores de superficie mayores a 1000m2\n",
                "mascara_coherencia_sup = data_limpia[\"surface_covered_in_m2\"] >= 1000\n",
                "data_coherencia_sup = data_limpia [mascara_coherencia_sup]\n",
                "data_coherencia_sup [[\"surface_total_in_m2\",\"surface_covered_in_m2\",\"Sup_m2_Clean_dos\", \"rooms\",\"property_type\"]]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_coherencia_sup.shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Para valores mayores a 1000m2, 39 filas, los valores en la columna superficie total son mas coherente que los datos de superficie cubierta, motivo por el cual en este caso no se modificará la variable"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# se hace mismo analisis par superficies menores a 1000m2\n",
                "mascara_coherencia_sup_menor = data_limpia[\"surface_covered_in_m2\"] <1000\n",
                "data_coherencia_sup_menor = data_consistencia_sup [mascara_coherencia_sup_menor]\n",
                "data_coherencia_sup_menor [[\"surface_total_in_m2\",\"surface_covered_in_m2\",\"Sup_m2_Clean_dos\", \"rooms\",\"property_type\"]].sample(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Para superficies cubiertas menores a 1000m2 la columna superficie cubierta posee valores mas razonales, valores que podrán luego imputarse para obtener un modelo mas preciso."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 3- Detección de outliers de la variable Superficie."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_limpia.boxplot(column= \"surface_total_in_m2\", by=\"property_type\") \n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "outlier_value = 10000\n",
                "mask_consistencia_outlier_sup= data_limpia[\"surface_total_in_m2\"] >= outlier_value\n",
                "data_limpia[mask_consistencia_outlier_sup].shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se analiza el contenido de las columnas para corroborar coherencia."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_consistencia_outliers = data_limpia[mask_consistencia_outlier_sup]\n",
                "data_consistencia_outliers[[\"surface_total_in_m2\",\"surface_covered_in_m2\"]].sample(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### Del análisis de las columnas donde existen outliers se detecta que los valores que se encuentran en la columna en superficie cubierta son mas coherentes que los de la columna superficie total, por lo cual, se decide para estas 87 columnas imputar el valor superficie cubierta como superficie total y revisar nuevamente outliers. Ver en apartado *\"IMPUTACION\"*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### 4- Se supone que para PH, apartments y store la superficie cubierta es la misma que la superficie total, *se descarta house entendiendo que estas pueden contar con \"patio\". con estos datos se reemplazan los valores faltantes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Joni aca iria el analisis gruopby por tipo de propiedad como son las superficies y sus desvios para ver si se cumple la hipotesis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Precio"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1-Extraer información con regex de columna descripción"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#generamos una copia del archivo para no modificar archivo base\n",
                "data_limpia= data.copy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#se pretende obtener informacion faltante en la columna precio, aplicando regex a la columna descripción, suponiendo que los valores de precios que nos interesan son en dolares (usd, u$d) seguido de varios digitos.\n",
                "serie_descripcion = data_limpia[\"description\"]\n",
                "\n",
                "pattern_usd = \"(?P<usd>(usd|u[$]d)\\s*?(\\d{2,}(,|.\\d+)?))\"\n",
                "pattern_usd_regex = re.compile(pattern_usd,  re.IGNORECASE)\n",
                "\n",
                "usd_match = serie_descripcion.apply(lambda x: x if x is np.NaN else pattern_usd_regex.search(x))\n",
                "\n",
                "mask_usd_match_notnull= usd_match.notnull()\n",
                "\n",
                "data_limpia.loc[mask_usd_match_notnull, \"usd_clean\"] = usd_match[mask_usd_match_notnull].apply(lambda x: x.group(0))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#la información obtenida posee caracteres no deseables por lo cual se vuelve a aplicar regex para poder dejar datos tipo float.\n",
                "serie_usd_clean=data_limpia[\"usd_clean\"]\n",
                "pattern_usd_dos=\"(?P<usddos>(\\d{2,}(,|.\\d+)?))\"\n",
                "pattern_usd_regex_dos=re.compile(pattern_usd_dos)\n",
                "usd_match_dos = serie_usd_clean.apply(lambda x: x if x is np.NaN else pattern_usd_regex_dos.search(x))\n",
                "mask_usd_match_notnull_dos= usd_match_dos.notnull()\n",
                "data_limpia.loc[mask_usd_match_notnull_dos, \"usd_clean_dos\"] = usd_match_dos[mask_usd_match_notnull_dos].apply(lambda x: x.group(\"usddos\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2-comprobar si las columnas precio ARS y USD pueden matchear"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#se corrobora primero la cantidad de datos nullos de ambas columnas\n",
                "#ademas se genera una mascara para corroborar si esos datos nulos coinciden, de manera tal que si lo hacen no se podrian matchear columnas.\n",
                "\n",
                "mask_ARS_empty=data_limpia[\"price_aprox_local_currency\"].isnull()\n",
                "mask_ARS_empty.value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mask_USD_empty=data_limpia[\"price_aprox_usd\"].isnull()\n",
                "mask_USD_empty.value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#se aplica mascara para ambas condiciones antes presentadas.\n",
                "\n",
                "mask_precio_empty=mask_ARS_empty&mask_USD_empty\n",
                "data_limpia[mask_precio_empty].shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### coinciden la cantidad de filas vacias en cada campo, por lo cual no es posible completar completar datos vacios de USD con ARS sacados de la columna \"price_aprox_local_currency\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3-Determinar la variacion precio con rooms y superficie"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "calculamos la correlacion entre variables con un grafico pairgrid, que relaciona las variables por pares"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#se cambian el tipo de datos a las variables numeros y se genera un nuevo dataframe solo con las variables de interes\n",
                "df_surf_float = data_limpia[\"surface_total_in_m2\"].astype(float) \n",
                "df_room_float = data_limpia['rooms'].astype(float) \n",
                "df_price_float = data_limpia['price_aprox_usd'].astype(float) \n",
                "df_tipo = data_limpia['property_type']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_tipo_usd_sup_room=pd.DataFrame((df_tipo,df_surf_float,df_room_float,df_price_float))\n",
                "data_tipo_usd_sup_room_T=data_tipo_usd_sup_room.T\n",
                "data_tipo_usd_sup_room_T.sample(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "g = sns.PairGrid(data_tipo_usd_sup_room_T, vars=['property_type','surface_total_in_m2','rooms','price_aprox_usd'],hue='property_type', palette='deep')\n",
                "g.map(plt.scatter, alpha=0.6)\n",
                "g.add_legend()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se puede ver una relacion entre el tipo de departamento  con el precio y la superficie total, no asi con la cantidad de habitaciones"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4- Se pretende determinar la relación entre tipo de propiedades y sus precios. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#para confirmar lo visto en el grafico agrupamos el precio por tipo de propiedad para determinar si el precio varia mucho con el tipo de propiedad\n",
                "data_agrupada_propiedaes= data_limpia.groupby('property_type')\n",
                "data_agrupada_propiedaes[['property_type','price_aprox_usd']].describe()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Joni si queres revisa la conclusion de estos casos"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Minar los datos"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Dar formato, limpiar, homogeneizar y filtrar los datos"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Decisiones (a tomar):\n",
                "> Ubicación\n",
                "> ___\n",
                "- en cuanto a ubicación quedarse con una columna geometry y el municipio (en el caso de Capital Federal, Comuna == municipio)\n",
                "  - En caso de geometry, usar lat-lon o geonames_id? porque geonames_id toma el punto medio del municipio (es decir, más inexacto) y lat-lon toma el punto exacto de la propiedad (es decir, más exacto) pero lat-lon tiene muchos valores faltantes.\n",
                "  - Una opción sería tomar la std media de lat-lon y geonames_id y modificar aleatoriamente geoname para que no quede un punto sobrerepresentado. (comencé a implementar como prueba de concepto pero no lo terminé)\n",
                "  - Se podría hacer algo similar con los 18640 datos faltantes de geonames_id.\n",
                "  - En el caso de municipio, habría que imputar unos 29842 valores que utilizan el barrio en vez del municipio ( se ve en places_data_4_elements). \n",
                "  - De esta manera en cuanto a municipio sólo perdemos 4780 filas que sólo tienen state_name y no municipio ni ciudad ni es posible imputarlos desde su georeferencia.\n",
                "> Superficie\n",
                "> ___\n",
                "- Queda definar los casos donde superficie cubierta es mayor que superficie total y si es posible imputar la superficie total a partir de la cubierta tomando la cubierta como la total a partir del promedio de superficie total por superficie cubierta.\n",
                "- Ver qué hacer con los outliers \n",
                "- Revisar celda 29 de superficie_limpia_columnas\n",
                "- Celda 34, discriminar casas y agregar superficie total a partir de la cubierta más un porcentaje de jardín (Jonathan)\n",
                "\n",
                "\n",
                "> Precio\n",
                ">- EVALUAR ELIMINAR LOS VALORES Q NO SON PESOS ARGENTINOS PARA PODER HACER UNA CONVERSION DE TODOS LOS VALORES A USD - PENDIENTE"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imputaciones ubicación"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Eliminamos las filas que no tienen municipio\n",
                "data_clean_location = data.drop(index=places_data_2_elements_geo.index)\n",
                "print(data_clean_location.shape)\n",
                "data_clean_location.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Corroboramos que eliminamos nuestro outlier de Colombia\n",
                "data_clean_location[data_clean_location['lat'] > 0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reemplazamos ciudades por municipios\n",
                "data_clean_location.loc[places_data_4_elements_with_mun_as_place_name.index, 'place_name'] = places_data_4_elements_with_mun_as_place_name['place_name']\n",
                "print(data_clean_location.shape)\n",
                "data_clean_location.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Elimino columnas con información duplicadas o innecesaria\n",
                "unnecesary_columns = ['country_name', 'place_with_parent_names']\n",
                "data_clean_location = data_clean_location.drop(columns=unnecesary_columns)\n",
                "print(data_clean_location.shape) \n",
                "data_clean_location.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# reutilizamos geo_location_data para obtener lat_geoname y lon_geoname\n",
                "print(geo_location_data.shape)\n",
                "geo_location_data.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# agregamos lat_geoname y lon_geoname a data_clean_location\n",
                "data_clean_location['lat_geoname'] = geo_location_data['lat_geoname']\n",
                "data_clean_location['lon_geoname'] = geo_location_data['lon_geoname']\n",
                "print(data_clean_location.shape)\n",
                "data_clean_location.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# limpiamos los datos que no tienen georeferencia\n",
                "data_clean_location.dropna(subset=[\"geonames_id\", 'lat_geoname', 'lon_geoname'], inplace=True)\n",
                "print(data_clean_location.shape)\n",
                "data_clean_location.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Corroboramos la cantidad de nulls en lat y lon\n",
                "data_clean_location[geolocation + ['lat_geoname', 'lon_geoname']].isna().sum() "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Veamos la la media de las lat y lon de la tabla original y los ingresados por geonames_id\n",
                "place_name_std = data_clean_location.groupby('place_name').agg({'lat':[ 'mean', 'std', 'count'], 'lon': ['mean', 'std',  'count'], 'lat_geoname':[ 'mean', 'std', 'count'], 'lon_geoname': ['mean', 'std', 'count']})\n",
                "place_name_std.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "place_name_std.columns = ['lat_mean', 'lat_std', 'lat_count', 'lon_mean', 'lon_std', 'lon_count', 'lat_geoname_mean', 'lat_geoname_std','lat_geoname_count', 'lon_geoname_mean', 'lon_geoname_std', 'lon_geoname_count'] \n",
                "place_name_std.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### para comparar eventualmente con los valores de referencia\n",
                "\n",
                "# Por cada place_name, en los casos que no tenemos lat y lon, imputamos la media de lat y lon \n",
                "# de geonames_id variada aleatoria con una desviación estándar de cada lugar.\n",
                "# for index, row in data_clean_location.iterrows():\n",
                "    #if np.isnan(row['lat']) and np.isnan(row['lon']) and \\\n",
                "    # if place_name_std['lat_geoname_count'][row['place_name']] > 10  and \\\n",
                "    # place_name_std['lon_geoname_count'][row['place_name']] > 10 :\n",
                "        # data_clean_location.loc[index, 'lat_inferred_for_testing'] = np.random.randn() * place_name_std['lat_std'][row['place_name']] * (1 if np.random.random() < 0.5 else -1 ) + place_name_std['lat_geoname_mean'][row['place_name']]\n",
                "       #  data_clean_location.loc[index, 'lon_inferred_for_testing'] = np.random.randn() * place_name_std['lon_std'][row['place_name']] * (1 if np.random.random() < 0.5 else -1 ) + place_name_std['lon_geoname_mean'][row['place_name']]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Por cada place_name, en los casos que no tenemos lat y lon, imputamos la media de lat y lon \n",
                "# de geonames_id variada aleatoria con una desviación estándar de cada lugar.\n",
                "for index, row in data_clean_location.iterrows():\n",
                "    if np.isnan(row['lat']) and np.isnan(row['lon']) and \\\n",
                "    place_name_std['lat_geoname_count'][row['place_name']] > 10  and \\\n",
                "    place_name_std['lon_geoname_count'][row['place_name']] > 10 :\n",
                "        data_clean_location.loc[index, 'lat_inferred'] = np.random.randn() * place_name_std['lat_std'][row['place_name']] * (1 if np.random.random() < 0.5 else -1 ) + place_name_std['lat_geoname_mean'][row['place_name']]\n",
                "        data_clean_location.loc[index, 'lon_inferred'] = np.random.randn() * place_name_std['lon_std'][row['place_name']] * (1 if np.random.random() < 0.5 else -1 ) + place_name_std['lon_geoname_mean'][row['place_name']]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ubicamos los puntos en el mapa para ver si se distribuyen de forma uniforme.\n",
                "\n",
                "# Estos df ya fueron creado y están puestos como referencia\n",
                "# geo_location_data_gdf = gpd.GeoDataFrame(geo_location_data, geometry=gpd.points_from_xy(geo_location_data.lon, geo_location_data.lat))\n",
                "# geo_location_data_gdf_capital = geo_location_data_gdf[data['state_name']=='Capital Federal']\n",
                "data_clean_location_capital = data_clean_location[data_clean_location['state_name']=='Capital Federal']\n",
                "\n",
                "geo_location_data_gdf_capital_geonames_norm = gpd.GeoDataFrame(data_clean_location_capital, geometry=gpd.points_from_xy(data_clean_location_capital.lon_inferred, data_clean_location_capital.lat_inferred))\n",
                "df_barrios_capital = pd.read_csv('./data/barrios.csv', sep=',', encoding='latin-1')\n",
                "\n",
                "\n",
                "df_barrios_capital[\"WKT\"] = df_barrios_capital[\"WKT\"].apply(shapely.wkt.loads) \n",
                "df_barrios_capital = gpd.GeoDataFrame(df_barrios_capital, geometry='WKT')\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10,20))\n",
                "\n",
                "geo_location_data_gdf_capital.plot(ax=ax, markersize=10, color='red', alpha=0.1) \n",
                "geo_location_data_gdf_geonames.plot(ax=ax, markersize=10, color='blue', alpha=0.1)\n",
                "geo_location_data_gdf_capital_geonames_norm.plot(ax=ax, markersize=10, color='green', alpha=0.1)\n",
                "df_barrios_capital.plot(ax=ax, alpha=0.4, color='grey', edgecolor='black')\n",
                "plt.xlim(-58.55,-58.350)\n",
                "plt.ylim(-34.705,-34.525) \n",
                "plt.legend(['lat-lon original', 'punto geonames_id', 'lat-lon inferida a partir de geonames'], loc='upper left')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vemos las propiedades en el mapa de Argentina\n",
                "argentina = world[world['name'] == 'Argentina']\n",
                "geo_location_data_gdf_norm = gpd.GeoDataFrame(data_clean_location, geometry=gpd.points_from_xy(data_clean_location.lon_inferred, data_clean_location.lat_inferred)) \n",
                "fig, ax = plt.subplots(figsize=(10,10))\n",
                "argentina.plot(ax=ax, color='grey', edgecolor='black')\n",
                "data_clean_location_gdf = gpd.GeoDataFrame(geo_location_data_gdf, geometry='geometry')\n",
                "data_clean_location_gdf.plot( markersize=1, color='red', alpha=0.1,ax=ax)\n",
                "geo_location_data_gdf_geonames.plot(ax=ax, markersize=1, color='blue', alpha=0.1)\n",
                "geo_location_data_gdf_norm.plot( markersize=1, color='green', alpha=0.1,ax=ax)\n",
                "plt.legend(['lat-lon original', 'punto geonames_id', 'lat-lon inferida a partir de geonames'], loc='upper left')\n",
                "plt.xlim(-72.5,-52.5)\n",
                "plt.ylim(-56,-20)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> Conclusión: \n",
                "> - Imputar place_name a partir de la información recolectada en places_with_parent_names es bastante preciso.\n",
                "> - Imputar lat y lon a partir de geonames_id es bastante más complicado y habría que ver cómo interactúa con el modelo. Si bien consideramos que vale la pena probarlo, sin mayor información no lo consideramos prudente para agregarlo al dataset\n",
                "> ___"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_clean_location"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_clean_location = data_clean_location.drop(columns=['geonames_id', 'lat', 'lon','lat-lon', \"lat_geoname\", \"lon_geoname\", \"lat_inferred\", \"lon_inferred\"])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(data_clean_location.shape)\n",
                "data_clean_location.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_limpia[\"precio_usd_limpio\"]=data_limpia[\"price_aprox_usd\"]\n",
                "data_limpia[\"precio_usd_limpio\"].fillna(data_limpia[\"usd_clean_dos\"],inplace=True)\n",
                "data_limpia[\"precio_usd_limpio\"].isnull().sum()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imputaciones a variable Superficie"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 1-se adicionan  la nueva columna \"sup_m2_total_limpia\" los datos obtenidos de la columna descripcion."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#recordamos cuantos datos vacíos tiene la columna de interés.\n",
                "data_limpia[\"surface_total_in_m2\"].isnull().sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_limpia[\"sup_m2_total_limpia\"] = data_limpia[\"surface_total_in_m2\"].copy()\n",
                "data_limpia[\"sup_m2_total_limpia\"].fillna(data_limpia[\"Sup_m2_Clean_tres\"],inplace=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(data_limpia[\"sup_m2_total_limpia\"].isnull().sum())\n",
                "(1-(data_limpia[\"sup_m2_total_limpia\"].isnull().sum()/data_limpia[\"surface_total_in_m2\"].isnull().sum()))*100"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#luego de la imputación de los datos obtenidos por regex la columna disminuye el 31% sus valores nulos."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 2- Del analisis de consistencia de datos entre columnas de superficie total vs cubierta, se decide imputar los valores de superficie cubierta a la columna de superficie total, para valores de superficie total menor que superficie cubierta y donde superficie cubierta es menor a 1000m2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_limpia.loc[mascara_coherencia_sup_menor,\"sup_m2_total_limpia\"] = data_limpia[\"surface_covered_in_m2\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(data_limpia[\"sup_m2_total_limpia\"].isnull().sum())\n",
                "(1-(data_limpia[\"sup_m2_total_limpia\"].isnull().sum()/data_limpia[\"surface_total_in_m2\"].isnull().sum()))*100"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "obteniendo una redución del 78.9% de la información faltante"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### 3- Del análisis de las columnas donde existen outliers se detecta que los valores en superficie cubierta son mas coherentes por lo cual, se decide para estas 87 columnas imputar el valor superficie cubierta como superficie total"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_limpia.loc[mask_consistencia_outlier_sup,\"sup_m2_total_limpia\"] = data_limpia[\"surface_covered_in_m2\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_limpia[\"sup_m2_total_limpia\"]=pd.to_numeric(data_limpia[\"sup_m2_total_limpia\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_limpia.boxplot(\"sup_m2_total_limpia\", by=\"property_type\") \n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "outlier_value = 10000\n",
                "mask_consistencia_outlier_sup_dos= data_limpia[\"sup_m2_total_limpia\"] >= outlier_value\n",
                "data[mask_consistencia_outlier_sup_dos].shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "consistencia_outlier_sup_dos= data_limpia[mask_consistencia_outlier_sup_dos]\n",
                "consistencia_outlier_sup_dos[[\"surface_total_in_m2\",\"surface_covered_in_m2\",\"sup_m2_total_limpia\", \"property_type\",\"rooms\",\"Sup_m2_Clean_tres\"]]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### 4- Se supone que para PH, apartments y store la superficie cubierta es la misma que la superficie total, *se descarta house entendiendo que estas pueden contar con \"patio\". con estos datos se reemplazan los valores faltantes "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# JONI ACA VA EL CODIGO QUE ARREGLATE VOS\n",
                "lo q esta aca abajo es lo que hice yo, q considera todo, o sea no excluye a \"house\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_key_not_null_mask = np.logical_and(data_limpia.property_type.notnull(), data.surface_covered_in_m2.notnull())\n",
                "data_key_not_null = data_limpia.loc[data_key_not_null_mask, :]\n",
                "data_key_not_null.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_key_not_null_grouped_tipo_sup= data_key_not_null.groupby([\"property_type\", \"surface_covered_in_m2\"])\n",
                "data_fillna= data_key_not_null_grouped_tipo_sup[\"surface_total_in_m2\"].transform(lambda grp: grp.fillna(grp.mean()))\n",
                "data_limpia[\"sup_m2_total_limpia\"].fillna(data_fillna,inplace=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_limpia[\"sup_m2_total_limpia\"].isnull().sum()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imputaciones a variable Precio"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 1- Imputación de información obtenida a través de regex"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "(1-data_limpia[\"precio_usd_limpio\"].isnull().sum()/data[\"price_aprox_usd\"].isnull().sum())*100"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se recuperó el 1.2% de la informacion de precios en dolares del campo descripcion. La recuperacion de datos no es significativa. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "####  2- Imputación a columna precios el valor del precio promedio por tipo de propiedad"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Se toma el valor medio del precio de propiedad por tipo de propiedad y se completan faltantes.\n",
                "data_key_not_null_mask_precio = np.logical_and(data_limpia.property_type.notnull(), data_limpia.price_aprox_usd.notnull())\n",
                "data_key_not_null_precio = data_limpia.loc[data_key_not_null_mask_precio, :]\n",
                "\n",
                "data_key_not_null_grouped_tipo_precio= data_key_not_null_precio.groupby([\"property_type\", 'price_aprox_usd'])\n",
                "\n",
                "\n",
                "data_fillna_precio= data_key_not_null_grouped_tipo_precio['price_aprox_usd'].transform(lambda grp: grp.fillna(grp.mean()))\n",
                "data_fillna_precio\n",
                "\n",
                "data_limpia[\"precio_usd_limpio\"].fillna(data_fillna_precio,inplace=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "(1-data_limpia[\"precio_usd_limpio\"].isnull().sum()/data[\"price_aprox_usd\"].isnull().sum())*100"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Se recupera el 67% de la informacion faltante "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_limpia[\"precio_usd_limpio\"].isnull().sum()/data_limpia.shape[0]*100"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "queda el 5.5% de la columna con faltantes."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# TODO: MEJORAR EL RANDOM DE LA IMPUTACIÓN DE GEONAMES_ID ALGUNOS QUEDAN MUY LEJOS. REPENSAR"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Por cada place_name, en los casos que no tenemos lat y lon, imputamos la media de lat y lon \n",
                "# de geonames_id variada aleatoria con una desviación estándar de cada lugar.\n",
                "\n",
                "\n",
                "for index, row in data_clean_location.iterrows():\n",
                "    if np.isnan(row['lat']) and np.isnan(row['lon']):\n",
                "        data_clean_location.loc[index, 'lat'] = np.random.randn() * place_name_std['lat_std'][row['place_name']] + place_name_std['lat_mean'][row['place_name']]\n",
                "        data_clean_location.loc[index, 'lon'] = np.random.randn() * place_name_std['lon_std'][row['place_name']] + place_name_std['lon_mean'][row['place_name']]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Corroboramos la cantidad de nulls en lat y lon\n",
                "data_clean_location[geolocation].isna().sum()  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ubicamos los puntos en el mapa para ver si se distribuyen de forma uniforme.\n",
                "\n",
                "# Estos df ya fueron creado y están puestos como referencia\n",
                "# geo_location_data_gdf = gpd.GeoDataFrame(geo_location_data, geometry=gpd.points_from_xy(geo_location_data.lon, geo_location_data.lat))\n",
                "# geo_location_data_gdf_capital = geo_location_data_gdf[data['state_name']=='Capital Federal']\n",
                "\n",
                "\n",
                "geo_location_data_gdf_capital_geonames_norm = gpd.GeoDataFrame(data_clean_location, geometry=gpd.points_from_xy(data_clean_location.lon, data_clean_location.lat))\n",
                "geo_location_data_gdf_capital['geometry']\n",
                "df_barrios_capital = pd.read_csv('./data/barrios.csv', sep=',', encoding='latin-1')\n",
                "\n",
                "\n",
                "df_barrios_capital[\"WKT\"] = df_barrios_capital[\"WKT\"].apply(shapely.wkt.loads) \n",
                "df_barrios_capital = gpd.GeoDataFrame(df_barrios_capital, geometry='WKT')\n",
                "\n",
                "geo_location_data_gdf_capital['geometry']\n",
                "fig, ax = plt.subplots(figsize=(10,20))\n",
                "\n",
                "geo_location_data_gdf_capital.plot(ax=ax, markersize=1, color='red', alpha=0.1) \n",
                "geo_location_data_gdf_capital_geonames_norm.plot(ax=ax, markersize=1, color='blue', alpha=0.1)\n",
                "df_barrios_capital.plot(ax=ax, alpha=0.4, color='grey', edgecolor='black')\n",
                "plt.xlim(-58.55,-58.350)\n",
                "plt.ylim(-34.705,-34.525) \n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Rearmamos la columna geometry con los nuevos valores de lat y lon\n",
                "data_clean_location['geometry'] = gpd.points_from_xy(data_clean_location.lon, data_clean_location.lat)\n",
                "# Limpiamos las columnas redundantes\n",
                "data_clean_location = data_clean_location.drop(columns=['lat', 'lon', 'lat_geoname', 'lon_geoname', 'geonames_id', 'lat-lon',])\n",
                "print(data_clean_location.shape)\n",
                "data_clean_location.head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vemos las propiedades en el mapa de Argentina\n",
                "argentina = world[world['name'] == 'Argentina'] \n",
                "fig, ax = plt.subplots(figsize=(10,10))\n",
                "argentina.plot(ax=ax, color='grey', edgecolor='black')\n",
                "data_clean_location_gdf = gpd.GeoDataFrame(data_clean_location, geometry='geometry')\n",
                "data_clean_location_gdf.plot(figsize=(10,10), markersize=1, color='red', alpha=0.1,ax=ax)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Refinar los datos"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exportar el nuevo dataset "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.4 ('DH')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.4"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "7ad4b00cfa812d91f92fb5dc88aa637f6cffee5ca8b01c2f389043adc33b2f6a"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
