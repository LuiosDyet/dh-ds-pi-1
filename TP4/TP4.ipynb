{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrantes:\n",
    "1. Camila Coltriani\n",
    "2. Luis Dartayet\n",
    "3. Irania Fuentes\n",
    "4. Jonathan Fichelson\n",
    "5. Ornella Cevoli\n",
    "# Trabajo práctico  4: Analisis de los sentimientos en Twitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "Las redes sociales como Twitter han demostrado ser excelentes recursos de información sobre muchos eventos que acontecen en el mundo; tienen el poder de cambiar las opiniones de millones de personas siendo especialmente útil para influir en las masas: campañas políticas, cotización de monedas virtuales, publicidad de ventas, entre otros.\n",
    "Pensando en esto, se presenta el siguiente objetivo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo:\n",
    "Analizar los sentimientos con la finalidad de predecir el comportamiento de personas y propagar cambios en tiempo real a medida que se desarrolla el evento que se quiere estudiar.\n",
    "\n",
    "## Fuente:\n",
    "Dataset Kaggle: https://www.kaggle.com/code/paoloripamonti/twitter-sentiment-analysis/input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score,plot_confusion_matrix,roc_auc_score, classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from scipy.stats import mode\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold,train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/twitter.csv', encoding='ANSI')\n",
    "data.columns = [ 'target', 'id', 'date', 'flag', 'user', 'text']\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis exploratorio de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nulls and duplicates\n",
    "print('Nulls: ', data.isnull().sum())\n",
    "print('Duplicates: ', data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove useless columns\n",
    "data.drop(['id', 'date', 'flag', 'user'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename target\n",
    "data['target'] = data['target'].map({0: 'negative', 4: 'positive'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target distribution\n",
    "print('Target distribution: ', data.target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set day of week as category\n",
    "data['date'] = data['date'].str[:3].astype('category')\n",
    "data['date'].cat.categories = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of tweets per day and target\n",
    "data.groupby(['date', 'target']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of tweets per day and target with a bar chart\n",
    "# {0: 'Negative', 2: 'Neutral', 4: 'Positive'}\n",
    "data.groupby(['date', 'target']).size().unstack().plot(kind='bar',  figsize=(10, 5))\n",
    "plt.title('Number of tweets per day and target')\n",
    "plt.xlabel('Day of week')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de texto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza del corpus\n",
    "\n",
    "### Remoción de Stopwords y aplicación de Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expresion regular para eliminar del corpus signos de puntación/ @/ direcciones electronicas #numeros\n",
    "limpieza_re = \"\\d+[^0-9]|@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remoción de Stopwords y aplicación de Stemming\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "#Funcion para limpiar el corpus\n",
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(limpieza_re, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "data.text = data.text.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Division de los datos en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asignamos las variables X e Y a modelar\n",
    "X = data.text\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividimos en train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usamos CountVectorizer para lleva nuestro corpus a una matriz de documentos y términos.\n",
    "vectorizer=CountVectorizer(strip_accents='unicode'); # hay palabras con acentos por error de tipeo o por ser palabras de otro idioma. Los removemos.\n",
    "# X_train=vectorizer.fit_transform(X_train);\n",
    "# X_test=vectorizer.transform(X_test);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuración del Pipeline\n",
    "\n",
    "PARA LEER: el primer pipe es con base a la clase 47_1 clasificacion de textos\n",
    "pipeline para varios modelos viene de aca https://www.youtube.com/watch?v=ES0lM7QBZnI&list=FLrNGJ7KGOB1XGjvk9OvahOw&index=1\n",
    "que está buena la idea, y lo que se haria luego es usar CV y tunear los hiperparametros sobre el mejor modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#armar un pipeline que incluya la verctorizacion y el clasificador para\n",
    "#optimizar sobre todos los hiperparámetros a la vez.\n",
    "\n",
    "#ttps://www.kaggle.com/code/balatmak/text-preprocessing-steps-and-universal-pipeline/notebook \n",
    "\n",
    "#https://www.kaggle.com/code/balatmak/text-classification-pipeline-newsgroups20/notebook esta notebook está buenisima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESTO ES UNA BASE \n",
    "\n",
    "#iniciamos el modelo\n",
    "#MNB = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_tfidf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer())])\n",
    "# tfidf_matrix = text_tfidf.fit_transform(texts_all_train)\n",
    "#https://www.kaggle.com/code/benboyet/web-mining-project-final-prediction/script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Código de pipe la verctorizacion y el clasificador para optimizar sobre todos los hiperparámetros a la vez con gridsearch\n",
    "\n",
    "pasos=[('vect',vectorizer),('classifier',MultinomialNB())]\n",
    "pipe=Pipeline(pasos)\n",
    "\n",
    "#Definimos TFIDvectorizer\n",
    "vectorizer=TfidfVectorizer(stop_words=stop_words,strip_accents='unicode'); #ngram_range=(1,2)\n",
    "\n",
    "#definimos la validacion cruzda\n",
    "skf=StratifiedKFold(n_splits=3,random_state=0,shuffle=True)\n",
    "\n",
    "#parametros del grid         ##usar otros valores\n",
    "params_grid={'classifier__alpha':[1,2,1.5],'vect__min_df':[1,3]}; #frecuencia menor al dado\n",
    "\n",
    "#entrenamos el modelo\n",
    "GS_CV=GridSearchCV(pipe,params_grid,cv=skf,verbose=1,n_jobs=-1);\n",
    "\n",
    "GS_CV.fit(X_train, y_train);\n",
    "\n",
    "print('best score:',GS_CV.best_score_)\n",
    "print('best params:',GS_CV.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_pred = GS_CV.predict(X_test)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, pipe_pred); \n",
    "plt.figure(figsize=(5, 2)); sns.heatmap(conf_matrix,  annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\"); plt.ylabel('True class'); plt.xlabel('Predicted class');plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reporte de clasificación\n",
    "print(classification_report(y_test, pipe_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo final con parametros seteados\n",
    "\n",
    "vectorizer_2=TfidfVectorizer(stop_words=stop_words,strip_accents='unicode', min_df=1);\n",
    "MNB = MultinomialNB(alpha=2,)\n",
    "pasos_2=[('vectorizer',vectorizer_2),('MNB',MNB)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configurar pipe\n",
    "pipe_2 = Pipeline(pasos_2)\n",
    "pipe_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_2_pred= pipe_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reporte de clasificación\n",
    "print(classification_report(y_test, pipe_2_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, pipe_2_pred); \n",
    "plt.figure(figsize=(5, 2)); sns.heatmap(conf_matrix,  annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\"); plt.ylabel('True class'); plt.xlabel('Predicted class');plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_2_pred= pipe_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"pipeline para varios modelos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline([('vect',vectorizer),\n",
    "                        ('lr_classifier',LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_knn = Pipeline([('vect',vectorizer),\n",
    "                        ('knn_classifier',KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista de pipelines\n",
    "pipelines = [pipeline_lr, pipeline_knn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy=0.0\n",
    "best_classifier=0\n",
    "best_pipeline=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diccionario de pipeline y tipos de clasificador para referencia\n",
    "pipe_dict = {0: \"Logistc Regresion\", 1:\"knn\"}\n",
    "\n",
    "#entrenamiento \n",
    "for pipe in pipelines:\n",
    "    pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(pipelines):\n",
    "    print(\"{} Accuracy para el conjunto de prubea: {}\".format(pipe_dict[i], model.score(X_test, y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACA AGREGO UN PIPELINE CON GRID SEARCH PARA ARBOL DE DECISIONES CON FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(stop_words=stop_words,strip_accents='unicode');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "pipeline_dt= Pipeline([('vect',vectorizer), ('tree', DecisionTreeClassifier()),('feature_selection', SelectFromModel(DecisionTreeClassifier()))])\n",
    "\n",
    "# Realizar la búsqueda de Grid Search con validación cruzada\n",
    "GS_dt=GridSearchCV(pipeline_dt,params_grid,cv=skf,scoring='accuracy', verbose=1,n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "GS_dt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la importancia de las características\n",
    "feature_importances = GS_dt.best_estimator_.named_steps['tree'].feature_importances_\n",
    "\n",
    "# Crear un DataFrame para mostrar la importancia de las características\n",
    "importances = pd.DataFrame({'feature': data.feature_names, 'importance': feature_importances})\n",
    "importances = importances.sort_values('importance', ascending=False)\n",
    "print(importances)\n",
    "\n",
    "# Imprimir los mejores parámetros encontrados por Grid Search\n",
    "print(GS_dt.best_params_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/mskorski/topic-extraction-and-visualization-musk-s-tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoción de stopwords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización/ Stemming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorización"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
