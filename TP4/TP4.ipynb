{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo:\n",
    "\n",
    "Predecir la probabilidad de que un individuo tweetee en un tweet positivo o negativo dependiendo del día de la semana.\n",
    "\n",
    "NO SE REALMENTE, NO ESTOY TAN SEGURO EN BASE AL GRÁFICO.\n",
    "\n",
    "\n",
    "\n",
    "_______________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from scipy.stats import mode\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold,train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BAJAR DATASET DE KAGGLE, RENOMBRARLO COMO TWITTER Y PONERLO EN LA CARPETA DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/twitter.csv', encoding='ANSI')\n",
    "data.columns = [ 'target', 'id', 'date', 'flag', 'user', 'text']\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get empty values\n",
    "data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set day of week as category\n",
    "data['date'] = data['date'].str[:3].astype('category')\n",
    "data['date'].cat.categories = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of tweets per day and target\n",
    "data.groupby(['date', 'target']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of tweets per day and target with a bar chart\n",
    "# {0: 'Negative', 2: 'Neutral', 4: 'Positive'}\n",
    "data.groupby(['date', 'target']).size().unstack().plot(kind='bar',  figsize=(10, 5))\n",
    "plt.title('Number of tweets per day and target')\n",
    "plt.xlabel('Day of week')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para la limpieza de texto\n",
    "limpieza_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para limpiar el corpus\n",
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(limpieza_re, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.text.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(data,stratify=data['target'],random_state=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train,test,y_train,y_test=train_test_split(data[\"text\"],data['target']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer=TfidfVectorizer();\n",
    "# X_train=vectorizer.fit_transform(train['text']);\n",
    "# y_train=train['target'];\n",
    "\n",
    "# X_test=vectorizer.transform(test['text']);\n",
    "# y_test=test['target'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(stop_words=stop_words,strip_accents='unicode');\n",
    "X_train=vectorizer.fit_transform(train['text']);\n",
    "y_train=train['target'];\n",
    "\n",
    "X_test=vectorizer.transform(test['text']);\n",
    "y_test=test['target'];\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf=StratifiedKFold(n_splits=3,random_state=0,shuffle=True)\n",
    "\n",
    "param_grid={'alpha':np.arange(0.05,1,0.05)};\n",
    "grid = GridSearchCV(MultinomialNB(), param_grid, cv=skf,verbose=1);\n",
    "grid.fit(X_train, y_train);\n",
    "print(\"Best cross-validation score: {:.4f}\".format(grid.best_score_));\n",
    "print(\"Best parameters: \", grid.best_params_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=Pipeline([('vect',vectorizer),('classifier',MultinomialNB())])\n",
    "\n",
    "params={'classifier__alpha':[0.1,0.5,1],'vect__min_df':[1,3]};\n",
    "\n",
    "GS_CV=GridSearchCV(model,params,cv=skf,verbose=1,n_jobs=-1);\n",
    "\n",
    "GS_CV=GridSearchCV(MultinomialNB(),params,cv=skf,verbose=1,n_jobs=3,refit=False)\n",
    "\n",
    "GS_CV.fit(train, y_train);\n",
    "\n",
    "print('best score:',GS_CV.best_score_)\n",
    "print('best params:',GS_CV.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoción de stopwords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización/ Stemming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorización"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
