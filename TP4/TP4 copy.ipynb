{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrantes:\n",
    "1. Camila Coltriani\n",
    "2. Luis Dartayet\n",
    "3. Irania Fuentes\n",
    "4. Jonathan Fichelson\n",
    "5. Ornella Cevoli\n",
    "# Trabajo práctico  4: Analisis de los sentimientos en Twitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro: Las redes sociales como Twitter han demostrado ser excelentes recursos de información sobre muchos eventos que acontecen en el mundo. Tiene el poder de cambiar las opiniones de millones de personas siendo especialmente útil para influir en las masas: campañas políticas, cotización de monedas virtuales, publicidad de ventas, entre otros.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo:\n",
    "El análisis de los sentimientos se puede utilizar para predecir el comportamiento de personas y propagar cambios en tiempo real a medida que se desarrolla el evento que se quiere estudiar.\n",
    "\n",
    "_______________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BAJAR DATASET DE KAGGLE, RENOMBRARLO COMO TWITTER Y PONERLO EN LA CARPETA DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599999, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag           user  \\\n",
       "0       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/twitter.csv', encoding='ANSI')\n",
    "data.columns = [ 'target', 'id', 'date', 'flag', 'user', 'text']\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls:  target    0\n",
      "id        0\n",
      "date      0\n",
      "flag      0\n",
      "user      0\n",
      "text      0\n",
      "dtype: int64\n",
      "Duplicates:  0\n"
     ]
    }
   ],
   "source": [
    "# Check nulls and duplicates\n",
    "print('Nulls: ', data.isnull().sum())\n",
    "print('Duplicates: ', data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove useless columns\n",
    "data.drop(['id', 'date', 'flag', 'user'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename target\n",
    "data['target'] = data['target'].map({0: 'negative', 4: 'positive'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution:  positive    800000\n",
      "negative    799999\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check target distribution\n",
    "print('Target distribution: ', data.target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asignamos las variables X e Y a modelar\n",
    "X = data.text\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1119999,)\n",
      "(480000,)\n",
      "(1119999,)\n",
      "(480000,)\n"
     ]
    }
   ],
   "source": [
    "#Dividimos en train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.3)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expresion regular para eliminar del corpus signos de puntación/ @/ direcciones electronicas #numeros\n",
    "limpieza_re = \"\\d+[^0-9]|@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]\"\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "#Funcion para limpiar el corpus\n",
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(limpieza_re, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessing(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_steps = [\n",
    "            ('text_processing', TextProcessing()),\n",
    "            ('vectorizer', CountVectorizer()),\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LR_pipe = [Pipeline( common_steps + [('lr', LogisticRegression())]), {'lr__C':[0.01,0.1,1], 'lr__penalty':['l2']}]\n",
    "DT_pipe = [Pipeline( common_steps + [('dt', tree.DecisionTreeClassifier())]), {'dt__max_depth':[ 8, 12, 16] }]\n",
    "MNB_pipe = [Pipeline( common_steps + [('mnb', MultinomialNB())]), {'mnb__alpha':[1, 10, 20]}]\n",
    "\n",
    "skf=StratifiedKFold(n_splits=3,random_state=0,shuffle=True)\n",
    "models = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "best score: 0.7749721205108219\n",
      "best params: {'lr__C': 0.1, 'lr__penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LuisD\\anaconda3\\envs\\dh\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "pipelines = [ LR_pipe]\n",
    "\n",
    "for pipe in pipelines:\n",
    "    GS_CV=GridSearchCV(pipe[0],pipe[1],cv=skf,verbose=10,n_jobs=3);\n",
    "    GS_CV.fit(X_train, y_train);\n",
    "    models.append(GS_CV)\n",
    "    print('best score:',GS_CV.best_score_)\n",
    "    print('best params:',GS_CV.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "best score: 0.5871737385479808\n",
      "best params: {'dt__max_depth': 12}\n"
     ]
    }
   ],
   "source": [
    "pipelines = [ DT_pipe]\n",
    "\n",
    "for pipe in pipelines:\n",
    "    GS_CV=GridSearchCV(pipe[0],pipe[1],cv=skf,verbose=10,n_jobs=3);\n",
    "    GS_CV.fit(X_train, y_train);\n",
    "    models.append(GS_CV)\n",
    "    print('best score:',GS_CV.best_score_)\n",
    "    print('best params:',GS_CV.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "best score: 0.765930148151918\n",
      "best params: {'mnb__alpha': 10}\n"
     ]
    }
   ],
   "source": [
    "pipelines = [ MNB_pipe]\n",
    "\n",
    "for pipe in pipelines:\n",
    "    GS_CV=GridSearchCV(pipe[0],pipe[1],cv=skf,verbose=10,n_jobs=3);\n",
    "    GS_CV.fit(X_train, y_train);\n",
    "    models.append(GS_CV)\n",
    "    print('best score:',GS_CV.best_score_)\n",
    "    print('best params:',GS_CV.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('text_processing', TextProcessing()),\n",
      "                ('vectorizer', CountVectorizer()),\n",
      "                ('lr', LogisticRegression(C=0.1))])\n",
      "Accuracy:  0.7774958333333334\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.75      0.77    240000\n",
      "    positive       0.76      0.81      0.78    240000\n",
      "\n",
      "    accuracy                           0.78    480000\n",
      "   macro avg       0.78      0.78      0.78    480000\n",
      "weighted avg       0.78      0.78      0.78    480000\n",
      "\n",
      "Confusion matrix:  [[179598  60402]\n",
      " [ 46400 193600]]\n",
      "Pipeline(steps=[('text_processing', TextProcessing()),\n",
      "                ('vectorizer', CountVectorizer()),\n",
      "                ('dt', DecisionTreeClassifier(max_depth=12))])\n",
      "Accuracy:  0.5881833333333333\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.23      0.35    240000\n",
      "    positive       0.55      0.95      0.70    240000\n",
      "\n",
      "    accuracy                           0.59    480000\n",
      "   macro avg       0.69      0.59      0.53    480000\n",
      "weighted avg       0.69      0.59      0.53    480000\n",
      "\n",
      "Confusion matrix:  [[ 54092 185908]\n",
      " [ 11764 228236]]\n",
      "Pipeline(steps=[('text_processing', TextProcessing()),\n",
      "                ('vectorizer', CountVectorizer()),\n",
      "                ('mnb', MultinomialNB(alpha=10))])\n",
      "Accuracy:  0.7689645833333333\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.79      0.77    240000\n",
      "    positive       0.78      0.75      0.76    240000\n",
      "\n",
      "    accuracy                           0.77    480000\n",
      "   macro avg       0.77      0.77      0.77    480000\n",
      "weighted avg       0.77      0.77      0.77    480000\n",
      "\n",
      "Confusion matrix:  [[189738  50262]\n",
      " [ 60635 179365]]\n"
     ]
    }
   ],
   "source": [
    "## Models evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    print(model.best_estimator_)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Classification report: ', classification_report(y_test, y_pred))\n",
    "    print('Confusion matrix: ', confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick best model\n",
    "best_model = models[0]\n",
    "for model in models:\n",
    "    if model.best_score_ > best_model.best_score_:\n",
    "        best_model = model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict new data\n",
    "\n",
    "new_data = ['supercalifragilisticexpialidocious']\n",
    "\n",
    "new_data = pd.Series(new_data)\n",
    "new_data = new_data.apply(preprocess)\n",
    "\n",
    "best_model.predict(new_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open model\n",
    "savedModel = None\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    savedModel = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model\n",
    "savedModel.predict(new_data)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
