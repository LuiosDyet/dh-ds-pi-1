{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrantes:\n",
    "1. Camila Coltriani\n",
    "2. Luis Dartayet\n",
    "3. Irania Fuentes\n",
    "4. Jonathan Fichelson\n",
    "5. Ornella Cevoli\n",
    "# Trabajo práctico  4: Analisis de los sentimientos en Twitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "Las redes sociales como Twitter han demostrado ser excelentes recursos de información sobre muchos eventos que acontecen en el mundo; tienen el poder de cambiar las opiniones de millones de personas siendo especialmente útil para influir en las masas: campañas políticas, cotización de monedas virtuales, publicidad de ventas, entre otros.\n",
    "Pensando en esto, se presenta el siguiente objetivo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo:\n",
    "Analizar los sentimientos con la finalidad de predecir el comportamiento de personas y propagar cambios en tiempo real a medida que se desarrolla el evento que se quiere estudiar.\n",
    "\n",
    "## Fuente:\n",
    "Dataset Kaggle: https://www.kaggle.com/code/paoloripamonti/twitter-sentiment-analysis/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Librerías\n",
    "## import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "## from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "# from sklearn.metrics import accuracy_score,plot_confusion_matrix,roc_auc_score, classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn import tree\n",
    "# from scipy.stats import mode\n",
    "# import seaborn as sns\n",
    "## import re\n",
    "\n",
    "# # nltk\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "## from nltk.corpus import stopwords\n",
    "## from  nltk.stem import SnowballStemmer\n",
    "## from nltk.stem import WordNetLemmatizer\n",
    "## from sklearn.feature_extraction.text import CountVectorizer,\n",
    "# TfidfTransformer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.model_selection import GridSearchCV,StratifiedKFold,train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "## from sklearn.pipeline import Pipeline\n",
    "\n",
    "# from sklearn.pipeline import make_pipeline, make_union\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.impute import SimpleImputer\n",
    "## from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599999, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag           user  \\\n",
       "0       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cargar y leer el dataset\n",
    "data = pd.read_csv('./data/twitter.csv', encoding='ANSI')\n",
    "data.columns = [ 'target', 'id', 'date', 'flag', 'user', 'text']\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis exploratorio de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls:  target    0\n",
      "id        0\n",
      "date      0\n",
      "flag      0\n",
      "user      0\n",
      "text      0\n",
      "dtype: int64\n",
      "Duplicates:  0\n"
     ]
    }
   ],
   "source": [
    "# Check nulls and duplicates\n",
    "print('Nulls: ', data.isnull().sum())\n",
    "print('Duplicates: ', data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove useless columns\n",
    "data.drop(['id', 'date', 'flag', 'user'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename target\n",
    "data['target'] = data['target'].map({0: 'negative', 4: 'positive'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution:  positive    800000\n",
      "negative    799999\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check target distribution\n",
    "print('Target distribution: ', data.target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asignamos las variables X e Y a modelar\n",
    "X = data.text\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de los datos\n",
    "\n",
    "### División del dataset en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1119999,)\n",
      "(480000,)\n",
      "(1119999,)\n",
      "(480000,)\n"
     ]
    }
   ],
   "source": [
    "#Dividimos en train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.3)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza del corpus: definición de una función para eliminar Stopword, aplicación de Stemming, convertir a minisculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expresion regular para eliminar del corpus signos de puntación/ @/ direcciones electronicas/ numeros\n",
    "limpieza_re = \"\\d+[^0-9]|@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]\"\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "#Funcion para limpiar el corpus\n",
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(limpieza_re, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definición de la clase de procesamiento de texto\n",
    "class TextProcessing(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración del pipeline y modelos a utilizar para la predicción de los sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_steps = [\n",
    "            ('text_processing', TextProcessing()),\n",
    "            ('vectorizer', CountVectorizer()),\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos a utilizar\n",
    "\n",
    "MNB_pipe = [Pipeline( common_steps + [('mnb', MultinomialNB())]), {'mnb__alpha':[1, 10, 20]}]\n",
    "KNN_pipe = [Pipeline( common_steps + [('knn', KNeighborsClassifier())]), {'knn__n_neighbors':[5]}]\n",
    "LR_pipe = [Pipeline( common_steps + [('lr', LogisticRegression())]), {'lr__C':[0.1,1,10], 'lr__penalty':['l2']}] #elimine l1 porque salia un error que solo soporta l2\n",
    "DT_pipe = [Pipeline( common_steps + [('dt', tree.DecisionTreeClassifier())]), {'dt__max_depth':[2, 4, 8] }]\n",
    "\n",
    "skf=StratifiedKFold(n_splits=3,random_state=0,shuffle=True)\n",
    "models = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "best score: 0.775319442249502\n",
      "best params: {'lr__C': 0.1, 'lr__penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\DH\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Configuración del gridsearch para el modelo de Regresión Logistica y definir los mejores parametros \n",
    "pipelines = [ LR_pipe]\n",
    "\n",
    "for pipe in pipelines:\n",
    "    GS_CV=GridSearchCV(pipe[0],pipe[1],cv=skf,verbose=10,n_jobs=3);\n",
    "    GS_CV.fit(X_train, y_train);\n",
    "    models.append(GS_CV)\n",
    "    print('best score:',GS_CV.best_score_)\n",
    "    print('best params:',GS_CV.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "best score: 0.5620621089840259\n",
      "best params: {'dt__max_depth': 8}\n"
     ]
    }
   ],
   "source": [
    "#Configuración del gridsearch para el modelo de arbol de decisión y definir los mejores parametros \n",
    "pipelines = [ DT_pipe]\n",
    "\n",
    "for pipe in pipelines:\n",
    "    GS_CV=GridSearchCV(pipe[0],pipe[1],cv=skf,verbose=10,n_jobs=3);\n",
    "    GS_CV.fit(X_train, y_train);\n",
    "    models.append(GS_CV)\n",
    "    print('best score:',GS_CV.best_score_)\n",
    "    print('best params:',GS_CV.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "best score: 0.7666203273395779\n",
      "best params: {'mnb__alpha': 10}\n"
     ]
    }
   ],
   "source": [
    "#Configuración del gridsearch para el modelo de Multinomial Naive Baye y definir los mejores parametros \n",
    "pipelines = [ MNB_pipe]\n",
    "\n",
    "for pipe in pipelines:\n",
    "    GS_CV=GridSearchCV(pipe[0],pipe[1],cv=skf,verbose=10,n_jobs=3);\n",
    "    GS_CV.fit(X_train, y_train);\n",
    "    models.append(GS_CV)\n",
    "    print('best score:',GS_CV.best_score_)\n",
    "    print('best params:',GS_CV.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    }
   ],
   "source": [
    "# iba por 30 min y no corrió\n",
    "# pipelines = [ KNN_pipe]\n",
    "\n",
    "# for pipe in pipelines:\n",
    "#     GS_CV=GridSearchCV(pipe[0],pipe[1],cv=skf,verbose=10,n_jobs=3);\n",
    "#     GS_CV.fit(X_train, y_train);\n",
    "#     models.append(GS_CV)\n",
    "#     print('best score:',GS_CV.best_score_)\n",
    "#     print('best params:',GS_CV.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'negative', ..., 'positive', 'positive',\n",
       "       'negative'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0].predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Estos parametros son para el primer modelo del pipe? MNB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7765958333333334\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.75      0.77    240000\n",
      "    positive       0.76      0.81      0.78    240000\n",
      "\n",
      "    accuracy                           0.78    480000\n",
      "   macro avg       0.78      0.78      0.78    480000\n",
      "weighted avg       0.78      0.78      0.78    480000\n",
      "\n",
      "Confusion matrix:  [[179404  60596]\n",
      " [ 46638 193362]]\n"
     ]
    }
   ],
   "source": [
    "## Evaluación del modelo \n",
    "from sklearn.metrics import accuracy_score,plot_confusion_matrix,roc_auc_score, classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "\n",
    "y_pred = models[0].predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Classification report: ', classification_report(y_test, y_pred))\n",
    "print('Confusion matrix: ', confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict new data\n",
    "new_data = ['supercalifragilisticexpialidocious']\n",
    "\n",
    "new_data = pd.Series(new_data)\n",
    "new_data = new_data.apply(preprocess)\n",
    "\n",
    "models[0].predict(new_data)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>Esta parte queda?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\LuisD\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>{'neg': 0.303, 'neu': 0.697, 'pos': 0.0, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>{'neg': 0.321, 'neu': 0.5, 'pos': 0.179, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>{'neg': 0.241, 'neu': 0.759, 'pos': 0.0, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                               text  \\\n",
       "0  negative  is upset that he can't update his Facebook by ...   \n",
       "1  negative  @Kenichan I dived many times for the ball. Man...   \n",
       "2  negative    my whole body feels itchy and like its on fire    \n",
       "3  negative  @nationwideclass no, it's not behaving at all....   \n",
       "4  negative                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                           sentiment  \n",
       "0  {'neg': 0.303, 'neu': 0.697, 'pos': 0.0, 'comp...  \n",
       "1  {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'comp...  \n",
       "2  {'neg': 0.321, 'neu': 0.5, 'pos': 0.179, 'comp...  \n",
       "3  {'neg': 0.241, 'neu': 0.759, 'pos': 0.0, 'comp...  \n",
       "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentIntensityAnalyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = sentimentIntensityAnalyzer.polarity_scores(sentence)\n",
    "    return score\n",
    "\n",
    "data['sentiment'] = data['text'].apply(sentiment_analyzer_scores)\n",
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
