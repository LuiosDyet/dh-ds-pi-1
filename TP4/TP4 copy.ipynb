{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrantes:\n",
    "1. Camila Coltriani\n",
    "2. Luis Dartayet\n",
    "3. Irania Fuentes\n",
    "4. Jonathan Fichelson\n",
    "5. Ornella Cevoli\n",
    "# Trabajo práctico  4: Analisis de los sentimientos en Twitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro: Las redes sociales como Twitter han demostrado ser excelentes recursos de información sobre muchos eventos que acontecen en el mundo. Tiene el poder de cambiar las opiniones de millones de personas siendo especialmente útil para influir en las masas: campañas políticas, cotización de monedas virtuales, publicidad de ventas, entre otros.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo:\n",
    "El análisis de los sentimientos se puede utilizar para predecir el comportamiento de personas y propagar cambios en tiempo real a medida que se desarrolla el evento que se quiere estudiar.\n",
    "\n",
    "_______________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Librerías\n",
    "## import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "## from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "# from sklearn.metrics import accuracy_score,plot_confusion_matrix,roc_auc_score, classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn import tree\n",
    "# from scipy.stats import mode\n",
    "# import seaborn as sns\n",
    "## import re\n",
    "\n",
    "# # nltk\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "## from nltk.corpus import stopwords\n",
    "## from  nltk.stem import SnowballStemmer\n",
    "## from nltk.stem import WordNetLemmatizer\n",
    "## from sklearn.feature_extraction.text import CountVectorizer,\n",
    "# TfidfTransformer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.model_selection import GridSearchCV,StratifiedKFold,train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "## from sklearn.pipeline import Pipeline\n",
    "\n",
    "# from sklearn.pipeline import make_pipeline, make_union\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.impute import SimpleImputer\n",
    "## from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BAJAR DATASET DE KAGGLE, RENOMBRARLO COMO TWITTER Y PONERLO EN LA CARPETA DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599999, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag           user  \\\n",
       "0       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/twitter.csv', encoding='ANSI')\n",
    "data.columns = [ 'target', 'id', 'date', 'flag', 'user', 'text']\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls:  target    0\n",
      "id        0\n",
      "date      0\n",
      "flag      0\n",
      "user      0\n",
      "text      0\n",
      "dtype: int64\n",
      "Duplicates:  0\n"
     ]
    }
   ],
   "source": [
    "# Check nulls and duplicates\n",
    "print('Nulls: ', data.isnull().sum())\n",
    "print('Duplicates: ', data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove useless columns\n",
    "data.drop(['id', 'date', 'flag', 'user'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename target\n",
    "data['target'] = data['target'].map({0: 'negative', 4: 'positive'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution:  positive    800000\n",
      "negative    799999\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check target distribution\n",
    "print('Target distribution: ', data.target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asignamos las variables X e Y a modelar\n",
    "X = data.text\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1119999,)\n",
      "(480000,)\n",
      "(1119999,)\n",
      "(480000,)\n"
     ]
    }
   ],
   "source": [
    "#Dividimos en train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.3)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expresion regular para eliminar del corpus signos de puntación/ @/ direcciones electronicas #numeros\n",
    "limpieza_re = \"\\d+[^0-9]|@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]\"\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "#Funcion para limpiar el corpus\n",
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(limpieza_re, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessing(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_steps = [\n",
    "            ('text_processing', TextProcessing()),\n",
    "            ('vectorizer', CountVectorizer()),\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MNB_pipe = [Pipeline( common_steps + [('mnb', MultinomialNB())]), {'mnb__alpha':[1, 10, 20]}]\n",
    "KNN_pipe = [Pipeline( common_steps + [('knn', KNeighborsClassifier())]), {'knn__n_neighbors':[5]}]\n",
    "LR_pipe = [Pipeline( common_steps + [('lr', LogisticRegression())]), {'lr__C':[0.1,1,10], 'lr__penalty':['l1','l2']}]\n",
    "DT_pipe = [Pipeline( common_steps + [('dt', tree.DecisionTreeClassifier())]), {'dt__max_depth':[2, 4, 8] }]\n",
    "\n",
    "skf=StratifiedKFold(n_splits=3,random_state=0,shuffle=True)\n",
    "models = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LuisD\\anaconda3\\envs\\dh\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "9 fits failed out of a total of 18.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\LuisD\\anaconda3\\envs\\dh\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\LuisD\\anaconda3\\envs\\dh\\lib\\site-packages\\sklearn\\pipeline.py\", line 382, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\LuisD\\anaconda3\\envs\\dh\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\LuisD\\anaconda3\\envs\\dh\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\LuisD\\anaconda3\\envs\\dh\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.77512569        nan 0.77399712        nan 0.77227301]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.7751256920765108\n",
      "best params: {'lr__C': 0.1, 'lr__penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LuisD\\anaconda3\\envs\\dh\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "pipelines = [ LR_pipe]\n",
    "\n",
    "for pipe in pipelines:\n",
    "    GS_CV=GridSearchCV(pipe[0],pipe[1],cv=skf,verbose=10,n_jobs=3);\n",
    "    GS_CV.fit(X_train, y_train);\n",
    "    models.append(GS_CV)\n",
    "    print('best score:',GS_CV.best_score_)\n",
    "    print('best params:',GS_CV.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "best score: 0.5626638952356208\n",
      "best params: {'dt__max_depth': 8}\n"
     ]
    }
   ],
   "source": [
    "pipelines = [ DT_pipe]\n",
    "\n",
    "for pipe in pipelines:\n",
    "    GS_CV=GridSearchCV(pipe[0],pipe[1],cv=skf,verbose=10,n_jobs=3);\n",
    "    GS_CV.fit(X_train, y_train);\n",
    "    models.append(GS_CV)\n",
    "    print('best score:',GS_CV.best_score_)\n",
    "    print('best params:',GS_CV.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "best score: 0.7662113984030343\n",
      "best params: {'mnb__alpha': 10}\n"
     ]
    }
   ],
   "source": [
    "pipelines = [ MNB_pipe]\n",
    "\n",
    "for pipe in pipelines:\n",
    "    GS_CV=GridSearchCV(pipe[0],pipe[1],cv=skf,verbose=10,n_jobs=3);\n",
    "    GS_CV.fit(X_train, y_train);\n",
    "    models.append(GS_CV)\n",
    "    print('best score:',GS_CV.best_score_)\n",
    "    print('best params:',GS_CV.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'dh (Python 3.10.4)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# pipelines = [ KNN_pipe]\n",
    "\n",
    "# for pipe in pipelines:\n",
    "#     GS_CV=GridSearchCV(pipe[0],pipe[1],cv=skf,verbose=10,n_jobs=3);\n",
    "#     GS_CV.fit(X_train, y_train);\n",
    "#     models.append(GS_CV)\n",
    "#     print('best score:',GS_CV.best_score_)\n",
    "#     print('best params:',GS_CV.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'negative', ..., 'positive', 'positive',\n",
       "       'negative'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7765958333333334\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.75      0.77    240000\n",
      "    positive       0.76      0.81      0.78    240000\n",
      "\n",
      "    accuracy                           0.78    480000\n",
      "   macro avg       0.78      0.78      0.78    480000\n",
      "weighted avg       0.78      0.78      0.78    480000\n",
      "\n",
      "Confusion matrix:  [[179404  60596]\n",
      " [ 46638 193362]]\n"
     ]
    }
   ],
   "source": [
    "## Evaluación del modelo\n",
    "\n",
    "from sklearn.metrics import accuracy_score,plot_confusion_matrix,roc_auc_score, classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "\n",
    "y_pred = models[0].predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Classification report: ', classification_report(y_test, y_pred))\n",
    "print('Confusion matrix: ', confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict new data\n",
    "\n",
    "new_data = ['supercalifragilisticexpialidocious']\n",
    "\n",
    "new_data = pd.Series(new_data)\n",
    "new_data = new_data.apply(preprocess)\n",
    "\n",
    "models[0].predict(new_data)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuración del Pipeline\n",
    "\n",
    "PARA LEER: el primer pipe es con base a la clase 47_1 clasificacion de textos\n",
    "pipeline para varios modelos viene de aca https://www.youtube.com/watch?v=ES0lM7QBZnI&list=FLrNGJ7KGOB1XGjvk9OvahOw&index=1\n",
    "que está buena la idea, y lo que se haria luego es usar CV y tunear los hiperparametros sobre el mejor modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\LuisD\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>{'neg': 0.303, 'neu': 0.697, 'pos': 0.0, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>{'neg': 0.321, 'neu': 0.5, 'pos': 0.179, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>{'neg': 0.241, 'neu': 0.759, 'pos': 0.0, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                               text  \\\n",
       "0  negative  is upset that he can't update his Facebook by ...   \n",
       "1  negative  @Kenichan I dived many times for the ball. Man...   \n",
       "2  negative    my whole body feels itchy and like its on fire    \n",
       "3  negative  @nationwideclass no, it's not behaving at all....   \n",
       "4  negative                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                           sentiment  \n",
       "0  {'neg': 0.303, 'neu': 0.697, 'pos': 0.0, 'comp...  \n",
       "1  {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'comp...  \n",
       "2  {'neg': 0.321, 'neu': 0.5, 'pos': 0.179, 'comp...  \n",
       "3  {'neg': 0.241, 'neu': 0.759, 'pos': 0.0, 'comp...  \n",
       "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentIntensityAnalyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = sentimentIntensityAnalyzer.polarity_scores(sentence)\n",
    "    return score\n",
    "\n",
    "data['sentiment'] = data['text'].apply(sentiment_analyzer_scores)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#armar un pipeline que incluya la verctorizacion y el clasificador para\n",
    "#optimizar sobre todos los hiperparámetros a la vez.\n",
    "\n",
    "#ttps://www.kaggle.com/code/balatmak/text-preprocessing-steps-and-universal-pipeline/notebook \n",
    "\n",
    "#https://www.kaggle.com/code/balatmak/text-classification-pipeline-newsgroups20/notebook esta notebook está buenisima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESTO ES UNA BASE \n",
    "\n",
    "#iniciamos el modelo\n",
    "#MNB = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_tfidf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer())])\n",
    "# tfidf_matrix = text_tfidf.fit_transform(texts_all_train)\n",
    "#https://www.kaggle.com/code/benboyet/web-mining-project-final-prediction/script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_pred = GS_CV.predict(X_test)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, pipe_pred); \n",
    "plt.figure(figsize=(5, 2)); sns.heatmap(conf_matrix,  annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\"); plt.ylabel('True class'); plt.xlabel('Predicted class');plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reporte de clasificación\n",
    "print(classification_report(y_test, pipe_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo final con parametros seteados\n",
    "\n",
    "vectorizer_2=TfidfVectorizer(stop_words=stop_words,strip_accents='unicode', min_df=1);\n",
    "MNB = MultinomialNB(alpha=2,)\n",
    "pasos_2=[('vectorizer',vectorizer_2),('MNB',MNB)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configurar pipe\n",
    "pipe_2 = Pipeline(pasos_2)\n",
    "pipe_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_2_pred= pipe_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reporte de clasificación\n",
    "print(classification_report(y_test, pipe_2_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, pipe_2_pred); \n",
    "plt.figure(figsize=(5, 2)); sns.heatmap(conf_matrix,  annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\"); plt.ylabel('True class'); plt.xlabel('Predicted class');plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_2_pred= pipe_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"pipeline para varios modelos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline([('vect',vectorizer),\n",
    "                        ('lr_classifier',LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_knn = Pipeline([('vect',vectorizer),\n",
    "                        ('knn_classifier',KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista de pipelines\n",
    "pipelines = [pipeline_lr, pipeline_knn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy=0.0\n",
    "best_classifier=0\n",
    "best_pipeline=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diccionario de pipeline y tipos de clasificador para referencia\n",
    "pipe_dict = {0: \"Logistc Regresion\", 1:\"knn\"}\n",
    "\n",
    "#entrenamiento \n",
    "for pipe in pipelines:\n",
    "    pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(pipelines):\n",
    "    print(\"{} Accuracy para el conjunto de prubea: {}\".format(pipe_dict[i], model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
